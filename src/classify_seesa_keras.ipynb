{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graduation Research "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seimei/usr/bin/virtualenv/tensorflow-with-gpu/lib/python3.5/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn import cross_validation\n",
    "import keras.callbacks\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "from keras.preprocessing import image\n",
    "import keras\n",
    "\n",
    "from keras.models import load_model\n",
    "import random as rn\n",
    "import os\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.models import model_from_json\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input \n",
    "#https://stackoverflow.com/questions/47555829/preprocess-input-method-in-keras                                                                                                            \n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "#from sklearn.decomposition import RandomizedPCA\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from PIL import ImageFile\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNISTのデータセットを利用して中間層出力検証方法の正確性を検証する(grad_cam検証)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 300, 300, 32)      896       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 300, 300, 32)      9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 150, 150, 32)      0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 150, 150, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 150, 150, 64)      18496     \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 150, 150, 64)      36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 75, 75, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 360000)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               184320512 \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 1026      \n",
      "=================================================================\n",
      "Total params: 184,387,106\n",
      "Trainable params: 184,387,106\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Tensor(\"Sum_6:0\", shape=(), dtype=float32)\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[[[0.0000000e+00 0.0000000e+00 1.2800000e+02]\n",
      "   [0.0000000e+00 0.0000000e+00 1.2800000e+02]\n",
      "   [0.0000000e+00 0.0000000e+00 1.2800000e+02]\n",
      "   ...\n",
      "   [0.0000000e+00 0.0000000e+00 1.2800000e+02]\n",
      "   [0.0000000e+00 0.0000000e+00 1.2800000e+02]\n",
      "   [0.0000000e+00 0.0000000e+00 1.2800000e+02]]\n",
      "\n",
      "  [[0.0000000e+00 0.0000000e+00 1.2800000e+02]\n",
      "   [0.0000000e+00 0.0000000e+00 1.2800000e+02]\n",
      "   [0.0000000e+00 0.0000000e+00 1.2800000e+02]\n",
      "   ...\n",
      "   [0.0000000e+00 0.0000000e+00 1.2800000e+02]\n",
      "   [0.0000000e+00 0.0000000e+00 1.2800000e+02]\n",
      "   [0.0000000e+00 0.0000000e+00 1.2800000e+02]]\n",
      "\n",
      "  [[0.0000000e+00 0.0000000e+00 1.2800000e+02]\n",
      "   [0.0000000e+00 0.0000000e+00 1.2800000e+02]\n",
      "   [0.0000000e+00 0.0000000e+00 1.2800000e+02]\n",
      "   ...\n",
      "   [0.0000000e+00 0.0000000e+00 1.2800000e+02]\n",
      "   [0.0000000e+00 0.0000000e+00 1.2800000e+02]\n",
      "   [0.0000000e+00 0.0000000e+00 1.2800000e+02]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[3.3921570e-01 3.3137256e-01 1.2833333e+02]\n",
      "   [2.6078433e-01 2.5294119e-01 1.2825098e+02]\n",
      "   [2.8235295e-01 2.8235295e-01 1.2827843e+02]\n",
      "   ...\n",
      "   [8.8235296e-02 1.5882353e-01 1.2801373e+02]\n",
      "   [1.4117648e-01 1.9803922e-01 1.2808432e+02]\n",
      "   [2.9411766e-01 3.4705883e-01 1.2824706e+02]]\n",
      "\n",
      "  [[3.0980393e-01 3.0980393e-01 1.2830980e+02]\n",
      "   [2.3725490e-01 2.4117647e-01 1.2823529e+02]\n",
      "   [3.3921570e-01 3.5294119e-01 1.2833922e+02]\n",
      "   ...\n",
      "   [8.2352944e-02 1.6078432e-01 1.2800000e+02]\n",
      "   [1.3333334e-01 2.0000000e-01 1.2805490e+02]\n",
      "   [3.4901962e-01 4.1176471e-01 1.2826863e+02]]\n",
      "\n",
      "  [[3.0392158e-01 3.0588236e-01 1.2829608e+02]\n",
      "   [4.3725491e-01 4.3725491e-01 1.2843333e+02]\n",
      "   [3.7647060e-01 3.8627452e-01 1.2837843e+02]\n",
      "   ...\n",
      "   [3.3137256e-01 4.0392157e-01 1.2822157e+02]\n",
      "   [1.5098040e-01 2.1764706e-01 1.2804903e+02]\n",
      "   [1.8627451e-01 2.5490198e-01 1.2809412e+02]]]]\n",
      "<PIL.Image.Image image mode=RGB size=300x300 at 0x7F5475926D30>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seimei/usr/bin/virtualenv/tensorflow-with-gpu/lib/python3.5/site-packages/ipykernel_launcher.py:350: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"conv2d_2_6/Relu:0\", shape=(?, 300, 300, 32), dtype=float32)\n",
      "[<keras.models.Sequential object at 0x7f547644fa20>, <keras.layers.core.Lambda object at 0x7f5475926b38>]\n",
      "<keras.models.Sequential object at 0x7f547644fa20>\n",
      "conv2d_2\n",
      "~~~~~~\n",
      "[<keras.layers.convolutional.Conv2D object at 0x7f547645ad68>]\n",
      "Tensor(\"conv2d_4_6/Relu:0\", shape=(?, 150, 150, 64), dtype=float32)\n",
      "[<keras.layers.convolutional.Conv2D object at 0x7f547644fa58>, <keras.layers.convolutional.Conv2D object at 0x7f547642b128>, <keras.layers.pooling.MaxPooling2D object at 0x7f547642beb8>, <keras.layers.core.Dropout object at 0x7f5476892ef0>, <keras.layers.convolutional.Conv2D object at 0x7f5476440c18>, <keras.layers.convolutional.Conv2D object at 0x7f547645ad68>, <keras.layers.pooling.MaxPooling2D object at 0x7f547642b0b8>, <keras.layers.core.Flatten object at 0x7f547645ab00>, <keras.layers.core.Dense object at 0x7f5475992da0>, <keras.layers.core.Dropout object at 0x7f54759ad6a0>, <keras.layers.core.Dense object at 0x7f54759ada20>]\n",
      "[<keras.models.Sequential object at 0x7f547644fa20>, <keras.layers.core.Lambda object at 0x7f5475926b38>]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Cannot interpret feed_dict key as Tensor: Can not convert a int into a Tensor.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/usr/bin/virtualenv/tensorflow-with-gpu/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1039\u001b[0m             subfeed_t = self.graph.as_graph_element(\n\u001b[0;32m-> 1040\u001b[0;31m                 subfeed, allow_tensor=True, allow_operation=False)\n\u001b[0m\u001b[1;32m   1041\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/usr/bin/virtualenv/tensorflow-with-gpu/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mas_graph_element\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   3338\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3339\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_as_graph_element_locked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/usr/bin/virtualenv/tensorflow-with-gpu/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_as_graph_element_locked\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   3427\u001b[0m       raise TypeError(\"Can not convert a %s into a %s.\" % (type(obj).__name__,\n\u001b[0;32m-> 3428\u001b[0;31m                                                            types_str))\n\u001b[0m\u001b[1;32m   3429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Can not convert a int into a Tensor.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-827e3407f8dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-827e3407f8dd>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    512\u001b[0m     \u001b[0mguided_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodify_backprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'GuidedBackProp'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m     \u001b[0msaliency_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompile_saliency_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mguided_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 514\u001b[0;31m     \u001b[0msaliency\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msaliency_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpreprocessed_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    515\u001b[0m     \u001b[0mgradcam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msaliency\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mheatmap\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m     \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"guided_gradcam.jpg\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeprocess_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradcam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/usr/bin/virtualenv/tensorflow-with-gpu/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2271\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2272\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2273\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2274\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/usr/bin/virtualenv/tensorflow-with-gpu/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/usr/bin/virtualenv/tensorflow-with-gpu/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1041\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m             raise TypeError(\n\u001b[0;32m-> 1043\u001b[0;31m                 'Cannot interpret feed_dict key as Tensor: ' + e.args[0])\n\u001b[0m\u001b[1;32m   1044\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot interpret feed_dict key as Tensor: Can not convert a int into a Tensor."
     ]
    }
   ],
   "source": [
    "#from sklearn.datasets import fetch_mldata\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input, decode_predictions\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import array_to_img, img_to_array, load_img\n",
    "\n",
    "from keras.layers.core import Lambda\n",
    "from keras.models import Sequential\n",
    "from tensorflow.python.framework import ops\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import keras \n",
    "import sys \n",
    "import cv2\n",
    "\n",
    "from keras.models import load_model\n",
    "# keep_dims is deprecated, use keepdims instead これに対する解決法                                  \n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "K.set_learning_phase(1)\n",
    "\n",
    "config = tf.ConfigProto(device_count={'GPU':1,'CPU':56})\n",
    "sess = tf.Session(config=config)\n",
    "keras.backend.set_session(sess)\n",
    "\n",
    "STANDARD_SIZE = (300, 300)\n",
    "\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "np.random.seed(0)\n",
    "rn.seed(0)\n",
    "\n",
    "#session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "tf.set_random_seed(0)\n",
    "\n",
    "input_shape = (300, 300, 3)\n",
    "batch_size = 4\n",
    "epochs = 10\n",
    "num_classes = 10\n",
    "\n",
    "f_log = './logs'\n",
    "f_model = './model'\n",
    "\n",
    "\n",
    "model_filename = 'cnn_model.json'\n",
    "weights_filename = 'cnn_model_weights.hdf5'\n",
    "\n",
    "\n",
    "def input_data(path_train, path_test):\n",
    "\n",
    "    x = []\n",
    "\n",
    "    with open(path_train, \"r\") as f:\n",
    "        train_path_list = f.readlines()\n",
    "\n",
    "    filenames = []\n",
    "    labels = []\n",
    "    for row in train_path_list:\n",
    "        row = row.split(\" \")\n",
    "        filenames.append(row[0])\n",
    "        labels.append(row[1])\n",
    "\n",
    "    label = []\n",
    "    for i in labels:\n",
    "        label.append(int(i))\n",
    "\n",
    "    for f in filenames:\n",
    "        x.append(image.img_to_array(img_to_matrix(f)))\n",
    "\n",
    "    x = np.asarray(x)\n",
    "    #正規化\n",
    "    x /= 255\n",
    "    y = np.asarray(label)\n",
    "    y = keras.utils.to_categorical(y, num_classes)\n",
    "\n",
    "    '==============================================================='\n",
    "\n",
    "    a = []\n",
    "\n",
    "    with open(path_test, \"r\") as f:\n",
    "        train_path_list = f.readlines()\n",
    "\n",
    "    filenames1 = []\n",
    "    labels1 = []\n",
    "    for row in train_path_list:\n",
    "        row = row.split(\" \")\n",
    "        filenames1.append(row[0])\n",
    "        labels1.append(row[1])\n",
    "\n",
    "    label1 = []\n",
    "    for i in labels1:\n",
    "        label1.append(int(i))\n",
    "\n",
    "    for f in filenames1:\n",
    "        a.append(image.img_to_array(img_to_matrix(f)))\n",
    "\n",
    "    test_data = np.asarray(a)\n",
    "    # 正規化\n",
    "    test_data /= 255\n",
    "    test_label = np.asarray(label1)\n",
    "    test_label = keras.utils.to_categorical(test_label, num_classes)\n",
    "\n",
    "    train_data, valid_data, train_label, valid_label = cross_validation.train_test_split(x, y, test_size=0.3)\n",
    "    test_data, valid1, test_label, valid2 = cross_validation.train_test_split(test_data, test_label, test_size=0.3)\n",
    "\n",
    "    return train_data, test_data, train_label, test_label, valid_data, valid_label\n",
    "\n",
    "# parse image\n",
    "def img_to_matrix(filename, verbose=False):\n",
    "    img = Image.open(filename)\n",
    "    if verbose:\n",
    "        print('changing size from %s to %s' % (str(img.size), str(STANDARD_SIZE)))\n",
    "    img = img.resize(STANDARD_SIZE)\n",
    "\n",
    "    imgArray = np.asarray(img)\n",
    "    return imgArray  # imgArray.shape = (167 x 300 x 3)\n",
    "\n",
    "\n",
    "# 1次元に引き延ばす(PCAで使用)\n",
    "def flatten_image(img):\n",
    "\n",
    "    s = img.shape[0] * img.shape[1] * img.shape[2]\n",
    "    img_wide = img.reshape(1, s)\n",
    "    return img_wide[0]\n",
    "\n",
    "\n",
    "def handle_image_with_pca(intermediate_output, y_test):\n",
    "    images = intermediate_output\n",
    "    labels = y_test\n",
    "    ls = []\n",
    "\n",
    "    for i in labels:\n",
    "        if i == 1:\n",
    "            ls.append(\"cloudy_seesaa\")\n",
    "        elif i == 0:\n",
    "            ls.append(\"sunny_seesaa\")\n",
    "    labels = ls\n",
    "\n",
    "    data = []\n",
    "    for image in images:\n",
    "        img = flatten_image(image)\n",
    "        data.append(img)\n",
    "\n",
    "    data = np.array(data)\n",
    "\n",
    "    is_train = np.random.uniform(0, 1, len(data)) <= 0.7\n",
    "    y = np.where(np.array(labels) == 'cloudy_seesaa', 1, 0)\n",
    "\n",
    "    train_x, train_y = data[is_train], y[is_train]\n",
    "\n",
    "    # plot in 2 dimensions\n",
    "    pca = RandomizedPCA(n_components=2)\n",
    "    X = pca.fit_transform(data)\n",
    "    df = pd.DataFrame({\"x\": X[:, 0], \"y\": X[:, 1],\n",
    "                       \"label\": np.where(y == 1, 'cloudy_seesaa', 'sunny_seesaa')})\n",
    "    colors = ['blue', 'red']\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for label, color in zip(df['label'].unique(), colors):\n",
    "        mask = df['label'] == label\n",
    "        plt.scatter(df[mask]['x'], df[mask]['y'], c=color, label=label)\n",
    "    sns.set()\n",
    "    plt.xlabel(\"pc1 (Principal Component1)\")  # 全データの分散が最大となる方向\n",
    "    plt.ylabel(\"pc2 (Principal Component2)\")  # 第一主成分に垂直な方向の軸\n",
    "    plt.legend()\n",
    "    #plt.show()\n",
    "    plt.savefig('pca_feature1.png')\n",
    "\n",
    "    # training a classifier\n",
    "    pca = RandomizedPCA(n_components=5)\n",
    "    train_x = pca.fit_transform(train_x)\n",
    "\n",
    "    svm = LinearSVC(C=1.0)\n",
    "    svm.fit(train_x, train_y)\n",
    "    joblib.dump(svm, 'model.pkl')\n",
    "\n",
    "    # evaluating the model\n",
    "    test_x, test_y = data[is_train == False], y[is_train == False]\n",
    "    test_x = pca.transform(test_x)\n",
    "    print(pd.crosstab(test_y, svm.predict(test_x),\n",
    "                      rownames=['Actual'], colnames=['Predicted']))\n",
    "\n",
    "def target_category_loss(x, category_index, nb_classes):\n",
    "    return tf.multiply(x, K.one_hot([category_index], nb_classes))\n",
    "\n",
    "def target_category_loss_output_shape(input_shape):\n",
    "    return input_shape\n",
    "\n",
    "def normalize(x):\n",
    "    # utility function to normalize a tensor by its L2 norm\n",
    "    return x / (K.sqrt(K.mean(K.square(x))) + 1e-5)\n",
    "\n",
    "def load_image(path):\n",
    "    img_path = path\n",
    "    img = image.load_img(img_path, target_size=(300, 300))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    return x\n",
    "\n",
    "def register_gradient():\n",
    "    if \"GuidedBackProp\" not in ops._gradient_registry._registry:\n",
    "        @ops.RegisterGradient(\"GuidedBackProp\")\n",
    "        def _GuidedBackProp(op, grad):\n",
    "            dtype = op.inputs[0].dtype\n",
    "            return grad * tf.cast(grad > 0., dtype) * \\\n",
    "                tf.cast(op.inputs[0] > 0., dtype)\n",
    "\n",
    "def compile_saliency_function(model, activation_layer='conv2d_4'):\n",
    "    input_img = model.input\n",
    "    layer_dict = dict([(layer.name, layer) for layer in model.layers[1:]])\n",
    "    layer_output = layer_dict[activation_layer].output\n",
    "    max_output = K.max(layer_output, axis=3)\n",
    "    saliency = K.gradients(K.sum(max_output), input_img)[0]\n",
    "    return K.function([input_img, K.learning_phase()], [saliency])\n",
    "\n",
    "def modify_backprop(model, name):\n",
    "    g = tf.get_default_graph()\n",
    "    with g.gradient_override_map({'Relu': name}):\n",
    "\n",
    "        # get layers that have an activation\n",
    "        layer_dict = [layer for layer in model.layers[1:]\n",
    "                      if hasattr(layer, 'activation')]\n",
    "\n",
    "        # replace relu activation\n",
    "        for layer in layer_dict:\n",
    "            if layer.activation == keras.activations.relu:\n",
    "                layer.activation = tf.nn.relu\n",
    "\n",
    "        # re-instanciate a new model\n",
    "        new_model = model\n",
    "    return new_model\n",
    "\n",
    "def deprocess_image(x):\n",
    "    '''\n",
    "    Same normalization as in:\n",
    "    https://github.com/fchollet/keras/blob/master/examples/conv_filter_visualization.py\n",
    "    '''\n",
    "    if np.ndim(x) > 3:\n",
    "        x = np.squeeze(x)\n",
    "    # normalize tensor: center on 0., ensure std is 0.1\n",
    "    x -= x.mean()\n",
    "    x /= (x.std() + 1e-5)\n",
    "    x *= 0.1\n",
    "\n",
    "    # clip to [0, 1]\n",
    "    x += 0.5\n",
    "    x = np.clip(x, 0, 1)\n",
    "\n",
    "    # convert to RGB array\n",
    "    x *= 255\n",
    "    if K.image_dim_ordering() == 'th':\n",
    "        x = x.transpose((1, 2, 0))\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x\n",
    "\n",
    "def grad_cam(input_model, image, category_index, layer_name):\n",
    "    model = Sequential()\n",
    "    model.add(input_model)\n",
    "\n",
    "    nb_classes = 2\n",
    "    target_layer = lambda x: target_category_loss(x, category_index, nb_classes)\n",
    "    model.add(Lambda(target_layer,\n",
    "                     output_shape = target_category_loss_output_shape))\n",
    "\n",
    "    loss = K.sum(model.layers[-1].output)\n",
    "    print(model.layers[0].layers[1].output)\n",
    "    print(model.layers)\n",
    "    print(model.layers[0])\n",
    "    print(model.layers[0].layers[1].name)\n",
    "    print(\"~~~~~~\")\n",
    "    print([l for l in  model.layers[0].layers if l.name == layer_name])\n",
    "    # 結論 is が反応してない。おそらくpythonのversionが原因か? python3.5 6 out\n",
    "    conv_output = [l for l in model.layers[0].layers if l.name == layer_name][0].output #model.layers[0].layers[1].output\n",
    "    #model.get_layer(layer_name).output\n",
    "    print(conv_output)\n",
    "       \n",
    "\n",
    "    grads = normalize(K.gradients(loss, conv_output)[0])\n",
    "    gradient_function = K.function([model.layers[0].input], [conv_output, grads])\n",
    "    print(model.layers[0].layers)\n",
    "    print(model.layers)\n",
    "    output, grads_val = gradient_function([image])\n",
    "    output, grads_val = output[0, :], grads_val[0, :, :, :]\n",
    "\n",
    "    weights = np.mean(grads_val, axis = (0, 1))\n",
    "    cam = np.ones(output.shape[0 : 2], dtype = np.float32)\n",
    "\n",
    "    for i, w in enumerate(weights):\n",
    "        cam += w * output[:, :, i]\n",
    "\n",
    "    cam = cv2.resize(cam, (300, 300))\n",
    "    cam = np.maximum(cam, 0)\n",
    "    heatmap = cam / np.max(cam)\n",
    "    \n",
    "    #Return to BGR [0..255] from the preprocessed image\n",
    "    image = image[0, :]\n",
    "    image -= np.min(image)\n",
    "    image = np.minimum(image, 255)\n",
    "\n",
    "    cam = cv2.applyColorMap(np.uint8(255*heatmap), cv2.COLORMAP_JET)\n",
    "    cam = np.float32(cam) + np.float32(image)\n",
    "    cam = 255 * cam / np.max(cam)\n",
    "    return np.uint8(cam), heatmap\n",
    "\n",
    "def Grad_Cam(input_model, x, layer_name):\n",
    "    '''\n",
    "    Args:\n",
    "       input_model: モデルオブジェクト\n",
    "       x: 画像(array)\n",
    "       layer_name: 畳み込み層の名前\n",
    "\n",
    "    Returns:\n",
    "       jetcam: 影響の大きい箇所を色付けした画像(array)\n",
    "\n",
    "    '''\n",
    " \n",
    "    # 前処理\n",
    "    X = np.expand_dims(x, axis=0)\n",
    "\n",
    "    X = X.astype('float32')\n",
    "    preprocessed_input = X / 255.0\n",
    " \n",
    "\n",
    "    # 予測クラスの算出\n",
    "\n",
    "    predictions = input_model.predict(X)\n",
    "    class_idx = np.argmax(predictions[0])\n",
    "    \n",
    "    #class_output = input_model.output[:, class_idx]\n",
    "    class_output =  K.sum(input_model.layers[-1].output)  \n",
    "    print(class_output)\n",
    "    \n",
    "    #  勾配を取得\n",
    "    conv_output = input_model.get_layer(layer_name).output   # layer_nameのレイヤーのアウトプット\n",
    "    grads = K.gradients(class_output, conv_output)[0]  # gradients(loss, variables) で、variablesのlossに関しての勾配を返す\n",
    "    gradient_function = K.function([input_model.input], [conv_output, grads])  # model.inputを入力すると、conv_outputとgradsを出力する関数\n",
    "\n",
    "    output, grads_val = gradient_function([X])\n",
    "    output, grads_val = output[0], grads_val[0]\n",
    "\n",
    "     # 重みを平均化して、レイヤーのアウトプットに乗じる\n",
    "    weights = np.mean(grads_val, axis=(0, 1))\n",
    "    cam = np.dot(output, weights)\n",
    "\n",
    "    print(cam)\n",
    "    # 画像化してヒートマップにして合成\n",
    "\n",
    "    cam = cv2.resize(cam, (300, 300), cv2.INTER_LINEAR) # 画像サイズは200で処理したので\n",
    "    cam = np.maximum(cam, 0) \n",
    "    cam = cam / cam.max()\n",
    "\n",
    "    jetcam = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_JET)  # モノクロ画像に疑似的に色をつける\n",
    "    jetcam = cv2.cvtColor(jetcam, cv2.COLOR_BGR2RGB)  # 色をRGBに変換\n",
    "    jetcam = (np.float32(jetcam) + X / 2)   # もとの画像に合成\n",
    "\n",
    "    return jetcam\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    \n",
    "    x_train, x_test, y_train, y_test, valid_data, valid_label = input_data(\"path_and_label_train.txt\",\n",
    "                                                                           \"path_and_label_test.txt\")\n",
    "\n",
    "    \"\"\"\n",
    "    mnist = fetch_mldata('MNIST original', data_home=\"./data_home\")\n",
    "    \n",
    "    x = mnist.data\n",
    "    y = mnist.target\n",
    "    y = keras.utils.to_categorical(y, num_classes)\n",
    "    \n",
    "    \n",
    "    x_train, x_test, y_train, y_test = cross_validation.train_test_split(x, y, test_size=0.3)\n",
    "\n",
    "    \n",
    "    old_session = KTF.get_session()\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        session = tf.Session('')\n",
    "        KTF.set_session(session)\n",
    "        KTF.set_learning_phase(1)\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                         activation='relu',\n",
    "                         input_shape=input_shape, kernel_initializer=\"he_normal\",\n",
    "                         bias_initializer=\"zeros\"))\n",
    "        model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        \n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(28, activation='relu', init=\"he_uniform\"))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(num_classes, activation='softmax')) # num_classes = 9値分類\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # load trained model\n",
    "    #json_string = open(os.path.join(f_model, model_filename)).read()\n",
    "    #model = model_from_json(json_string)\n",
    "    #model.load_weights(os.path.join(f_model, weights_filename))\n",
    "    model = load_model(\"./model/Mymodel.h5\")\n",
    "    print(model.summary())\n",
    "\n",
    "    \"\"\"\n",
    "        model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                      optimizer=\"SGD\",\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "        print(model.summary())\n",
    "\n",
    "        # callback function\n",
    "        tb_cb = keras.callbacks.TensorBoard(log_dir=f_log, histogram_freq=1)\n",
    "        cbks = [tb_cb]\n",
    "\n",
    "        # train\n",
    "        history = model.fit(x_train, y_train,\n",
    "                            batch_size=batch_size,\n",
    "                            epochs=epochs,\n",
    "                            verbose=1,       #進行状況の表示モード\n",
    "                            callbacks=cbks,  # [plot_losses, csv_logger],\n",
    "                            validation_data=(x_test, y_test))\n",
    "        score_train = model.evaluate(x_train,y_train, verbose=1, batch_size=4) \n",
    "        score_test = model.evaluate(x_test, y_test, verbose=1, batch_size=4)\n",
    "        print('Train loss: {0}'.format(score_train[0]))\n",
    "        print('Train accuracy: {0}'.format(score_train[1]))\n",
    "        print('Test loss: {0}'.format(score_test[0]))\n",
    "        print('Test accuracy: {0}'.format(score_test[1]))\n",
    "        \n",
    "        # 学習済みモデル書き出し\n",
    "        json_string = model.to_json()\n",
    "        open(os.path.join(f_model, 'cnn_model.json'), 'w').write(json_string)\n",
    "        yaml_string = model.to_yaml()\n",
    "        open(os.path.join(f_model, 'cnn_model.yaml'), 'w').write(yaml_string)\n",
    "        print('save weights')\n",
    "        model.save_weights(os.path.join(f_model, 'cnn_model_weights.hdf5'))\n",
    "\n",
    "        # modelのlayer_nameを調べる\n",
    "        for layer in model.layers:\n",
    "            print(layer.name)\n",
    "            \n",
    "        layer_name =\"conv2d_2\"#\"dropout_1\"#\"conv2d_2\"# \"max_pooling2d_1\"\n",
    "        intermediate_layer_model = keras.models.Model(inputs=model.input,\n",
    "                                                      outputs=model.get_layer(layer_name).output)\n",
    "       \n",
    "        layers = model.layers[1:2]#[3:4]#[1:2]#[2:3]\n",
    "        img = image.load_img(\"/home/seimei/Graduation_Research/dataset_valid/hare/class3-1/image_0064.jpg\", target_size=(300, 300))\n",
    "        img = image.img_to_array(img)\n",
    "        img /= 255\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "        # 指定したlayer_nameと一致するレイヤーの出力を取得する\n",
    "        activations = intermediate_layer_model.predict(img)\n",
    "        activations = [activation for layer, activation in zip(layers, activations) if isinstance(layer, Conv2D)]\n",
    "        print(activations)\n",
    "        # 単品の特徴画像生成#\n",
    "        for i, activation in enumerate(activations):\n",
    "            num_of_image = activation.shape[2]\n",
    "            max = np.max(activation[0])\n",
    "            for j in range(0, num_of_image):\n",
    "                plt.figure(figsize=(50, 50))\n",
    "                sns.heatmap(activation[:, :,j], vmin=0, vmax=max, xticklabels=False, yticklabels=False, square=False)\n",
    "                plt.savefig(\"%d_%d.png\" % (i+1, j+1))\n",
    "                plt.close()\n",
    "        # 出力層ごとに特徴画像を並べてヒートマップ画像として出力\n",
    "        for i, activation in enumerate(activations):\n",
    "            num_of_image = activation.shape[2]\n",
    "            cols = math.ceil(math.sqrt(num_of_image))\n",
    "            rows = math.floor(num_of_image / cols)\n",
    "            screen = []\n",
    "            for y in range(0, rows):\n",
    "                row = []\n",
    "                for x in range(0, cols):\n",
    "                    j = y * cols + x\n",
    "                    if j < num_of_image:\n",
    "                        row.append(activation[:, :, j])\n",
    "                    else:\n",
    "                        row.append(np.zeros())\n",
    "                screen.append(np.concatenate(row, axis=1))\n",
    "            screen = np.concatenate(screen, axis=0)\n",
    "            plt.figure(figsize=(50, 50))\n",
    "            sns.heatmap(screen, xticklabels=False, yticklabels=False)\n",
    "            name = \"maxpooling2d\"\n",
    "            plt.savefig(\"%s.png\" % name)\n",
    "            plt.close()\n",
    "        \"\"\"\n",
    "    img = image.load_img(\"/home/seimei/Graduation_Research/dataset_valid/hare/class3-1/image_0001.jpg\", target_size=(300, 300))\n",
    "    img = image.img_to_array(img)\n",
    "    #img.astype('float32')\n",
    "    img /= 255.0\n",
    "\n",
    "    preprocessed_input = load_image(\"/home/seimei/Graduation_Research/dataset_valid/hare/class3-1/image_0001.jpg\")\n",
    "    predictions=model.predict(np.expand_dims(img, axis=0))\n",
    "    predicted_class= np.argmax(predictions)\n",
    "    #print(predicted_class)\n",
    "    #print(predictions)\n",
    "    \n",
    "    imag = Grad_Cam(model, img, 'conv2d_4')\n",
    "    print(imag)\n",
    "    image1=image.array_to_img(np.squeeze(imag, axis=0))\n",
    "    print(image1)\n",
    "    #cv2.imshow(\"image\", image1)\n",
    "    image1.save(\"./image.jpg\")\n",
    "    #cv2.imwrite('image.jpg',image1)\n",
    "    \n",
    "    cam, heatmap = grad_cam(model, np.expand_dims(img, axis=0), predicted_class, \"conv2d_4\")\n",
    "    cv2.imwrite(\"gradcam.jpg\", cam)\n",
    "\n",
    "    register_gradient()\n",
    "    guided_model = modify_backprop(model, 'GuidedBackProp')\n",
    "    saliency_fn = compile_saliency_function(guided_model)\n",
    "    saliency = saliency_fn([preprocessed_input, 0])\n",
    "    gradcam = saliency[0] * heatmap[..., np.newaxis]\n",
    "    cv2.imwrite(\"guided_gradcam.jpg\", deprocess_image(gradcam))\n",
    "        #handle_image_with_pca(activations, np.zeros(1))\n",
    "    #KTF.set_session(old_session)# add for TeonsorBoard\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1枚の画像に対しPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "config = tf.ConfigProto(device_count={'GPU':1,'CPU':56})\n",
    "sess = tf.Session(config=config)\n",
    "keras.backend.set_session(sess)\n",
    "\n",
    "STANDARD_SIZE = (36, 36)\n",
    "\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "np.random.seed(0)\n",
    "rn.seed(0)\n",
    "\n",
    "#session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "tf.set_random_seed(0)\n",
    "\n",
    "input_shape = (36, 36, 3)\n",
    "batch_size = 4\n",
    "epochs = 30\n",
    "num_classes = 2\n",
    "\n",
    "f_log = './logs'\n",
    "f_model = './model'\n",
    "\n",
    "\n",
    "model_filename = 'cnn_model.json'\n",
    "weights_filename = 'cnn_model_weights.hdf5'\n",
    "\n",
    "\n",
    "def input_data(path_train, path_test):\n",
    "\n",
    "    x = []\n",
    "\n",
    "    with open(path_train, \"r\") as f:\n",
    "        train_path_list = f.readlines()\n",
    "\n",
    "    filenames = []\n",
    "    labels = []\n",
    "    for row in train_path_list:\n",
    "        row = row.split(\" \")\n",
    "        filenames.append(row[0])\n",
    "        labels.append(row[1])\n",
    "\n",
    "    label = []\n",
    "    for i in labels:\n",
    "        label.append(int(i))\n",
    "\n",
    "    for f in filenames:\n",
    "        x.append(image.img_to_array(img_to_matrix(f)))\n",
    "\n",
    "    x = np.asarray(x)\n",
    "    #正規化\n",
    "    x /= 255\n",
    "    y = np.asarray(label)\n",
    "    y = keras.utils.to_categorical(y, num_classes)\n",
    "\n",
    "    '==============================================================='\n",
    "\n",
    "    a = []\n",
    "\n",
    "    with open(path_test, \"r\") as f:\n",
    "        train_path_list = f.readlines()\n",
    "\n",
    "    filenames1 = []\n",
    "    labels1 = []\n",
    "    for row in train_path_list:\n",
    "        row = row.split(\" \")\n",
    "        filenames1.append(row[0])\n",
    "        labels1.append(row[1])\n",
    "\n",
    "    label1 = []\n",
    "    for i in labels1:\n",
    "        label1.append(int(i))\n",
    "\n",
    "    for f in filenames1:\n",
    "        a.append(image.img_to_array(img_to_matrix(f)))\n",
    "\n",
    "    test_data = np.asarray(a)\n",
    "    # 正規化\n",
    "    test_data /= 255\n",
    "    test_label = np.asarray(label1)\n",
    "    test_label = keras.utils.to_categorical(test_label, num_classes)\n",
    "\n",
    "    train_data, valid_data, train_label, valid_label = cross_validation.train_test_split(x, y, test_size=0.3)\n",
    "    test_data, valid1, test_label, valid2 = cross_validation.train_test_split(test_data, test_label, test_size=0.3)\n",
    "\n",
    "    return train_data, test_data, train_label, test_label, valid_data, valid_label\n",
    "\n",
    "\n",
    "\n",
    "# parse image\n",
    "def img_to_matrix(filename, verbose=False):\n",
    "    img = Image.open(filename)\n",
    "    if verbose:\n",
    "        print('changing size from %s to %s' % (str(img.size), str(STANDARD_SIZE)))\n",
    "    img = img.resize(STANDARD_SIZE)\n",
    "\n",
    "    imgArray = np.asarray(img)\n",
    "    return imgArray  # imgArray.shape = (167 x 300 x 3)\n",
    "\n",
    "\n",
    "# 1次元に引き延ばす(PCAで使用)\n",
    "def flatten_image(img):\n",
    "\n",
    "    s = img.shape[0] * img.shape[1] * img.shape[2]\n",
    "    img_wide = img.reshape(1, s)\n",
    "    return img_wide[0]\n",
    "\n",
    "\n",
    "def handle_image_with_pca(intermediate_output, y_test):\n",
    "    images = intermediate_output\n",
    "    labels = y_test\n",
    "    ls = []\n",
    "\n",
    "    for i in labels:\n",
    "        if i == 1:\n",
    "            ls.append(\"cloudy_seesaa\")\n",
    "        elif i == 0:\n",
    "            ls.append(\"sunny_seesaa\")\n",
    "    labels = ls\n",
    "\n",
    "    data = []\n",
    "    for image in images:\n",
    "        img = flatten_image(image)\n",
    "        data.append(img)\n",
    "\n",
    "    data = np.array(data)\n",
    "\n",
    "    is_train = np.random.uniform(0, 1, len(data)) <= 0.7\n",
    "    y = np.where(np.array(labels) == 'cloudy_seesaa', 1, 0)\n",
    "\n",
    "    train_x, train_y = data[is_train], y[is_train]\n",
    "\n",
    "    # plot in 2 dimensions\n",
    "    pca = RandomizedPCA(n_components=2)\n",
    "    X = pca.fit_transform(data)\n",
    "    df = pd.DataFrame({\"x\": X[:, 0], \"y\": X[:, 1],\n",
    "                       \"label\": np.where(y == 1, 'cloudy_seesaa', 'sunny_seesaa')})\n",
    "    colors = ['blue', 'red']\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for label, color in zip(df['label'].unique(), colors):\n",
    "        mask = df['label'] == label\n",
    "        plt.scatter(df[mask]['x'], df[mask]['y'], c=color, label=label)\n",
    "    sns.set()\n",
    "    plt.xlabel(\"pc1 (Principal Component1)\")  # 全データの分散が最大となる方向\n",
    "    plt.ylabel(\"pc2 (Principal Component2)\")  # 第一主成分に垂直な方向の軸\n",
    "    plt.legend()\n",
    "    #plt.show()\n",
    "    plt.savefig('pca_feature1.png')\n",
    "\n",
    "    # training a classifier\n",
    "    pca = RandomizedPCA(n_components=5)\n",
    "    train_x = pca.fit_transform(train_x)\n",
    "\n",
    "    svm = LinearSVC(C=1.0)\n",
    "    svm.fit(train_x, train_y)\n",
    "    joblib.dump(svm, 'model.pkl')\n",
    "\n",
    "    # evaluating the model\n",
    "    test_x, test_y = data[is_train == False], y[is_train == False]\n",
    "    test_x = pca.transform(test_x)\n",
    "    print(pd.crosstab(test_y, svm.predict(test_x),\n",
    "                      rownames=['Actual'], colnames=['Predicted']))\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    x_train, x_test, y_train, y_test, valid_data, valid_label = input_data(\"path_and_label_train.txt\",\n",
    "                                                                           \"path_and_label_test.txt\")\n",
    "\n",
    "    old_session = KTF.get_session()\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        session = tf.Session('')\n",
    "        KTF.set_session(session)\n",
    "        KTF.set_learning_phase(1)\n",
    "        \"\"\"\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(64, kernel_size=(3, 3),\n",
    "                         activation='relu',\n",
    "                         input_shape=input_shape, kernel_initializer=\"he_normal\",\n",
    "                         bias_initializer=\"zeros\",\n",
    "                        padding='same', name='block1_conv1'))\n",
    "        model.add(Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2'))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2),padding='same', name='block1_pool'))\n",
    "        model.add(Conv2D(128,(3,3), activation='relu',padding='same', name='block2_conv1'))\n",
    "        model.add(Conv2D(128,(3,3), activation='relu',padding='same', name='block2_conv2'))\n",
    "        model.add(MaxPooling2D(pool_size=(2,2),padding='same', name='block2_pool'))        \n",
    "        model.add(Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1'))\n",
    "        model.add(Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2'))\n",
    "        model.add(Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3'))\n",
    "        model.add(MaxPooling2D((2, 2), strides=(2, 2), padding='same', name='block3_pool'))\n",
    "        model.add(Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1'))\n",
    "        model.add(Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2'))\n",
    "        model.add(Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3'))\n",
    "        model.add(MaxPooling2D((2, 2), strides=(2, 2), padding='same', name='block4_pool'))\n",
    "        model.add(Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1'))\n",
    "        model.add(Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv2'))\n",
    "        model.add(Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv3'))\n",
    "        model.add(MaxPooling2D((2, 2), strides=(2, 2), padding='same', name='block5_pool'))\n",
    "\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(4096, activation='relu', name='fc1'))\n",
    "        model.add(Dropout(0.5, name='dropout1'))\n",
    "\n",
    "        model.add(Dense(4096, activation='relu',name='fc2' ))\n",
    "        model.add(Dropout(0.5, name='dropout2'))\n",
    "        model.add(Dense(num_classes, activation='softmax')) # num_classes = 2値分類\n",
    "\n",
    "        \"\"\"\n",
    "        # load trained model\n",
    "        json_string = open(os.path.join(f_model, model_filename)).read()\n",
    "        model = model_from_json(json_string)\n",
    "        model.load_weights(os.path.join(f_model, weights_filename))\n",
    "        \"\"\"\n",
    "        model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                      optimizer=\"SGD\",\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "        print(model.summary())\n",
    "\n",
    "        # callback function\n",
    "        tb_cb = keras.callbacks.TensorBoard(log_dir=f_log, histogram_freq=1)\n",
    "        cbks = [tb_cb]\n",
    "\n",
    "        # train\n",
    "        history = model.fit(x_train, y_train,\n",
    "                            batch_size=batch_size,\n",
    "                            epochs=epochs,\n",
    "                            verbose=1,       #進行状況の表示モード\n",
    "                            callbacks=cbks,  # [plot_losses, csv_logger],\n",
    "                            validation_data=(x_test, y_test))\n",
    "        score_train = model.evaluate(x_train,y_train, verbose=1, batch_size=4) \n",
    "        score_test = model.evaluate(x_test, y_test, verbose=1, batch_size=4)\n",
    "        print('Train loss: {0}'.format(score_train[0]))\n",
    "        print('Train accuracy: {0}'.format(score_train[1]))\n",
    "        print('Test loss: {0}'.format(score_test[0]))\n",
    "        print('Test accuracy: {0}'.format(score_test[1]))\n",
    "        \n",
    "        # 学習済みモデル書き出し\n",
    "        json_string = model.to_json()\n",
    "        open(os.path.join(f_model, 'cnn_model.json'), 'w').write(json_string)\n",
    "        yaml_string = model.to_yaml()\n",
    "        open(os.path.join(f_model, 'cnn_model.yaml'), 'w').write(yaml_string)\n",
    "        print('save weights')\n",
    "        model.save_weights(os.path.join(f_model, 'cnn_model_weights.hdf5'))\n",
    "        \"\"\"\n",
    "        # modelのlayer_nameを調べる\n",
    "        for layer in model.layers:\n",
    "            print(layer.name)\n",
    "\n",
    "        layer_name =\"block5_conv3\"#\"dropout_1\"#\"conv2d_2\"# \"max_pooling2d_1\"\n",
    "        intermediate_layer_model = keras.models.Model(inputs=model.input,\n",
    "                                                      outputs=model.get_layer(layer_name).output)\n",
    "       \n",
    "        layers = model.layers[16:17]#[3:4]#[1:2]#[2:3]\n",
    "        img = image.load_img(\"/home/seimei/Graduation_Research/dataset_valid/hare/class3-1/image_0064.jpg\", target_size=(36, 36))\n",
    "        img = image.img_to_array(img)\n",
    "        img /= 255\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "        # 指定したlayer_nameと一致するレイヤーの出力を取得する\n",
    "        activations = intermediate_layer_model.predict(img)\n",
    "        activations = [activation for layer, activation in zip(layers, activations) if isinstance(layer, Conv2D)]\n",
    "        print(activations)\n",
    "        # 単品の特徴画像生成#\n",
    "        \"\"\"\n",
    "        for i, activation in enumerate(activations):\n",
    "            num_of_image = activation.shape[2]\n",
    "            max = np.max(activation[0])\n",
    "            for j in range(0, num_of_image):\n",
    "                plt.figure(figsize=(50, 50))\n",
    "                sns.heatmap(activation[:, :,j], vmin=0, vmax=max, xticklabels=False, yticklabels=False, square=False)\n",
    "                plt.savefig(\"%d_%d.png\" % (i+1, j+1))\n",
    "                plt.close()\n",
    "        \"\"\"\n",
    "        # 出力層ごとに特徴画像を並べてヒートマップ画像として出力\n",
    "        for i, activation in enumerate(activations):\n",
    "            num_of_image = activation.shape[2]\n",
    "            cols = math.ceil(math.sqrt(num_of_image))\n",
    "            rows = math.floor(num_of_image / cols)\n",
    "            screen = []\n",
    "            for y in range(0, rows):\n",
    "                row = []\n",
    "                for x in range(0, cols):\n",
    "                    j = y * cols + x\n",
    "                    if j < num_of_image:\n",
    "                        row.append(activation[:, :, j])\n",
    "                    else:\n",
    "                        row.append(np.zeros())\n",
    "                screen.append(np.concatenate(row, axis=1))\n",
    "            screen = np.concatenate(screen, axis=0)\n",
    "            plt.figure(figsize=(50, 50))\n",
    "            sns.heatmap(screen, xticklabels=False, yticklabels=False)\n",
    "            name = \"maxpooling2d\"\n",
    "            plt.savefig(\"%s.png\" % name)\n",
    "            plt.close()\n",
    "        y = keras.utils.to_categorical(np.r_[np.zeros(512), np.ones(512)], num_classes)\n",
    "        handle_image_with_pca(activations, y)#np.zeros(1))\n",
    "    # add for TeonsorBoard\n",
    "\n",
    "    KTF.set_session(old_session)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2枚の画像に対しPCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conv2D_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(388, 300, 300, 3)\n",
      "(388, 2)\n",
      "WARNING:tensorflow:From /home/seimei/usr/bin/virtualenv/tensorflow-with-gpu/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:1205: calling reduce_prod (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/seimei/usr/bin/virtualenv/tensorflow-with-gpu/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:2755: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/seimei/usr/bin/virtualenv/tensorflow-with-gpu/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:1290: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seimei/usr/bin/virtualenv/tensorflow-with-gpu/lib/python3.5/site-packages/ipykernel_launcher.py:203: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(512, activation=\"relu\", kernel_initializer=\"he_uniform\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 300, 300, 32)      896       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 300, 300, 32)      9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 150, 150, 32)      0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 150, 150, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 150, 150, 64)      18496     \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 150, 150, 64)      36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 75, 75, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 75, 75, 128)       73856     \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 75, 75, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 37, 37, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 175232)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               89719296  \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 1026      \n",
      "=================================================================\n",
      "Total params: 90,007,330\n",
      "Trainable params: 90,007,330\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 388 samples, validate on 354 samples\n",
      "Epoch 1/20\n",
      "388/388 [==============================] - 175s - loss: 0.6157 - acc: 0.6753 - val_loss: 0.7187 - val_acc: 0.6243\n",
      "Epoch 2/20\n",
      "388/388 [==============================] - 175s - loss: 0.4333 - acc: 0.8351 - val_loss: 0.6238 - val_acc: 0.6949\n",
      "Epoch 3/20\n",
      "388/388 [==============================] - 177s - loss: 0.1509 - acc: 0.9356 - val_loss: 1.0040 - val_acc: 0.6243\n",
      "Epoch 4/20\n",
      "388/388 [==============================] - 177s - loss: 0.0691 - acc: 0.9742 - val_loss: 1.1114 - val_acc: 0.6215\n",
      "Epoch 5/20\n",
      "388/388 [==============================] - 177s - loss: 0.0026 - acc: 1.0000 - val_loss: 1.2386 - val_acc: 0.6554\n",
      "Epoch 6/20\n",
      "388/388 [==============================] - 177s - loss: 8.0095e-04 - acc: 1.0000 - val_loss: 1.3777 - val_acc: 0.6554\n",
      "Epoch 7/20\n",
      "388/388 [==============================] - 177s - loss: 0.0014 - acc: 1.0000 - val_loss: 1.8809 - val_acc: 0.5650\n",
      "Epoch 8/20\n",
      "388/388 [==============================] - 177s - loss: 0.0019 - acc: 1.0000 - val_loss: 1.4322 - val_acc: 0.6808\n",
      "Epoch 9/20\n",
      "388/388 [==============================] - 177s - loss: 2.7365e-04 - acc: 1.0000 - val_loss: 1.5005 - val_acc: 0.6497\n",
      "Epoch 10/20\n",
      "388/388 [==============================] - 177s - loss: 7.0411e-04 - acc: 1.0000 - val_loss: 1.9203 - val_acc: 0.6243\n",
      "Epoch 11/20\n",
      "388/388 [==============================] - 177s - loss: 1.5227e-04 - acc: 1.0000 - val_loss: 1.7138 - val_acc: 0.6525\n",
      "Epoch 12/20\n",
      "388/388 [==============================] - 193s - loss: 1.5940e-04 - acc: 1.0000 - val_loss: 1.9418 - val_acc: 0.6215\n",
      "Epoch 13/20\n",
      "388/388 [==============================] - 203s - loss: 2.3950e-04 - acc: 1.0000 - val_loss: 1.4390 - val_acc: 0.6780\n",
      "Epoch 14/20\n",
      "388/388 [==============================] - 194s - loss: 2.3370e-04 - acc: 1.0000 - val_loss: 1.7593 - val_acc: 0.6384\n",
      "Epoch 15/20\n",
      "388/388 [==============================] - 193s - loss: 4.1697e-05 - acc: 1.0000 - val_loss: 1.7789 - val_acc: 0.6215\n",
      "Epoch 16/20\n",
      "388/388 [==============================] - 193s - loss: 1.8236e-04 - acc: 1.0000 - val_loss: 1.7118 - val_acc: 0.6525\n",
      "Epoch 17/20\n",
      "116/388 [=======>......................] - ETA: 78s - loss: 1.4947e-04 - acc: 1.0000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-d6e8f41b4c4f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-d6e8f41b4c4f>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    229\u001b[0m                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m       \u001b[0;31m#進行状況の表示モード\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcbks\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# [plot_losses, csv_logger],\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m                             validation_data=(x_test, y_test))                                                               \n\u001b[0m\u001b[1;32m    232\u001b[0m         \u001b[0mscore_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0mscore_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/usr/bin/virtualenv/tensorflow-with-gpu/lib/python3.5/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    865\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 867\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    868\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m~/usr/bin/virtualenv/tensorflow-with-gpu/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1596\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1597\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1598\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1599\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1600\u001b[0m     def evaluate(self, x, y,\n",
      "\u001b[0;32m~/usr/bin/virtualenv/tensorflow-with-gpu/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/usr/bin/virtualenv/tensorflow-with-gpu/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2271\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2272\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2273\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2274\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/usr/bin/virtualenv/tensorflow-with-gpu/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/usr/bin/virtualenv/tensorflow-with-gpu/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1100\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1101\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/usr/bin/virtualenv/tensorflow-with-gpu/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1272\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1273\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/usr/bin/virtualenv/tensorflow-with-gpu/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1276\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/usr/bin/virtualenv/tensorflow-with-gpu/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1261\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1263\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/usr/bin/virtualenv/tensorflow-with-gpu/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "config = tf.ConfigProto(device_count={'GPU':1,'CPU':56})\n",
    "sess = tf.Session(config=config)\n",
    "keras.backend.set_session(sess)\n",
    "\n",
    "STANDARD_SIZE = (300, 300)\n",
    "\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "np.random.seed(0)\n",
    "rn.seed(0)\n",
    "\n",
    "#session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)                              \n",
    "tf.set_random_seed(0)\n",
    "\n",
    "input_shape = (300, 300, 3)\n",
    "batch_size = 4\n",
    "epochs = 20\n",
    "num_classes = 2\n",
    "\n",
    "f_log = './logs'\n",
    "f_model = './model'\n",
    "\n",
    "\n",
    "model_filename = 'cnn_model.json'\n",
    "weights_filename = 'cnn_model_weights.hdf5'\n",
    "\n",
    "def input_data(path_train, path_test):\n",
    "\n",
    "    x = []\n",
    "\n",
    "    with open(path_train, \"r\") as f:\n",
    "        train_path_list = f.readlines()\n",
    "\n",
    "    filenames = []\n",
    "    labels = []\n",
    "    for row in train_path_list:\n",
    "        row = row.split(\" \")\n",
    "        filenames.append(row[0])\n",
    "        labels.append(row[1])\n",
    "\n",
    "    label = []\n",
    "    for i in labels:\n",
    "        label.append(int(i))\n",
    "\n",
    "    for f in filenames:\n",
    "        x.append(image.img_to_array(img_to_matrix(f)))\n",
    "\n",
    "    x = np.asarray(x)\n",
    "    #正規化                                                                                                                 \n",
    "    x /= 255\n",
    "    y = np.asarray(label)\n",
    "    y = keras.utils.to_categorical(y, num_classes)\n",
    "\n",
    "    '==============================================================='\n",
    "\n",
    "    a = []\n",
    "    \n",
    "    with open(path_test, \"r\") as f:\n",
    "        train_path_list = f.readlines()\n",
    "\n",
    "    filenames1 = []\n",
    "    labels1 = []\n",
    "    for row in train_path_list:\n",
    "        row = row.split(\" \")\n",
    "        filenames1.append(row[0])\n",
    "        labels1.append(row[1])\n",
    "\n",
    "    label1 = []\n",
    "    for i in labels1:\n",
    "        label1.append(int(i))\n",
    "\n",
    "    for f in filenames1:\n",
    "        a.append(image.img_to_array(img_to_matrix(f)))\n",
    "\n",
    "    test_data = np.asarray(a)\n",
    "    # 正規化                                                                                                                \n",
    "    test_data /= 255\n",
    "    test_label = np.asarray(label1)\n",
    "    test_label = keras.utils.to_categorical(test_label, num_classes)\n",
    "\n",
    "    train_data, valid_data, train_label, valid_label = cross_validation.train_test_split(x, y, test_size=0.3)\n",
    "    test_data, valid1, test_label, valid2 = cross_validation.train_test_split(test_data, test_label, test_size=0.3)\n",
    "    \n",
    "    print(np.shape(train_data))\n",
    "    print(np.shape(train_label))\n",
    "    return train_data, test_data, train_label, test_label, valid_data, valid_label\n",
    "\n",
    "\n",
    "\n",
    "# parse image                                                                                                               \n",
    "def img_to_matrix(filename, verbose=False):\n",
    "    img = Image.open(filename)\n",
    "    if verbose:\n",
    "        print('changing size from %s to %s' % (str(img.size), str(STANDARD_SIZE)))\n",
    "    img = img.resize(STANDARD_SIZE)\n",
    "    imgArray = np.asarray(img)\n",
    "    return imgArray  # imgArray.shape = (167 x 300 x 3)                                                                     \n",
    "\n",
    "\n",
    "# 1次元に引き延ばす(PCAで使用)                                                                                              \n",
    "def flatten_image(img):\n",
    "\n",
    "    s = img.shape[0] * img.shape[1] #* img.shape[2]\n",
    "    img_wide = img.reshape(1, s)\n",
    "    return img_wide[0]\n",
    "\n",
    "\n",
    "def handle_image_with_pca(activations, y_test):\n",
    "\n",
    "    for i in range(2):\n",
    "        images = activations[i,:,:,:]\n",
    "        print(images)\n",
    "        if i == 0:\n",
    "            labels = y_test[:64]\n",
    "        elif i == 1:\n",
    "            labels = y_test[64:]\n",
    "        ls = []\n",
    "\n",
    "        for label in labels:\n",
    "\n",
    "            if list(label) == [0,1]:\n",
    "                ls.append(\"cloudy_seesaa\")\n",
    "            elif list(label) == [1,0]:\n",
    "                ls.append(\"sunny_seesaa\")\n",
    "        labels = ls\n",
    "        print(labels)\n",
    "        data = []\n",
    "        for a in range(64):#for image in images:\n",
    "            img = flatten_image(images[:, :, a])\n",
    "            data.append(img)\n",
    "\n",
    "        data = np.array(data)\n",
    "\n",
    "        is_train = np.random.uniform(0, 1, len(data)) <= 0.7\n",
    "        y = np.where(np.array(labels) == 'cloudy_seesaa', 1, 0)\n",
    "\n",
    "        train_x, train_y = data[is_train], y[is_train]\n",
    "\n",
    "        pca = PCA(n_components=2)\n",
    "        X = pca.fit_transform(data)\n",
    "        if i == 0:\n",
    "            df1 = pd.DataFrame({\"x\": X[:, 0], \"y\": X[:, 1],\n",
    "                               \"label\": np.where(y == 1, 'cloudy_seesaa', 'sunny_seesaa')})\n",
    "        elif i == 1:\n",
    "            df2 = pd.DataFrame({\"x\": X[:, 0], \"y\": X[:, 1],\n",
    "                               \"label\": np.where(y == 1, 'cloudy_seesaa', 'sunny_seesaa')})\n",
    "            df = pd.concat([df1, df2])\n",
    "    colors = ['red','blue']\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for label, color in zip(df['label'].unique(), colors):\n",
    "        mask = df['label'] == label\n",
    "        plt.scatter(df[mask]['x'], df[mask]['y'], c=color, label=label)\n",
    "    sns.set()\n",
    "    plt.xlabel(\"pc1 (Principal Component1)\")  # 全データの分散が最大となる方向                                              \n",
    "    plt.ylabel(\"pc2 (Principal Component2)\")  # 第一主成分に垂直な方向の軸                                                  \n",
    "    plt.legend()\n",
    "    #plt.show()                                                                                                             \n",
    "    plt.savefig('pca_feature1.png')\n",
    "\n",
    "    # training a classifier                                                                                                 \n",
    "    pca =PCA(n_components=2)\n",
    "    train_x = pca.fit_transform(train_x)\n",
    "    \"\"\"\n",
    "    svm = LinearSVC(C=1.0)\n",
    "    svm.fit(train_x, train_y)\n",
    "    joblib.dump(svm, 'model.pkl')\n",
    "\n",
    "    # evaluating the model                                                                                                  \n",
    "    test_x, test_y = data[is_train == False], y[is_train == False]\n",
    "    test_x = pca.transform(test_x)\n",
    "    print(pd.crosstab(test_y, svm.predict(test_x),\n",
    "                      rownames=['Actual'], colnames=['Predicted']))\n",
    "    \"\"\"\n",
    "def main():\n",
    "\n",
    "    x_train, x_test, y_train, y_test, valid_data, valid_label = input_data(\"path_and_label_train.txt\",\n",
    "                                                                           \"path_and_label_test.txt\")\n",
    "\n",
    "    old_session = KTF.get_session()\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        session = tf.Session('')\n",
    "        KTF.set_session(session)\n",
    "        KTF.set_learning_phase(1)\n",
    "\n",
    "        model = Sequential()                                                                                                \n",
    "        model.add(Conv2D(32, kernel_size=(3, 3), padding='same', activation='relu', input_shape=input_shape, kernel_initializer=\"he_normal\", bias_initializer=\"zeros\"))                                                                         \n",
    "        model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))                                                                    \n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(64, kernel_size=(3,3), activation='relu', padding='same'))\n",
    "        model.add(Conv2D(64, kernel_size=(3,3), activation='relu', padding='same'))\n",
    "        model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "        model.add(Conv2D(128, kernel_size=(3,3), activation='relu', padding='same'))\n",
    "        model.add(Conv2D(128, kernel_size=(3,3), activation='relu', padding='same'))\n",
    "        model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "        \n",
    "    \n",
    "        model.add(Flatten())                                                                                                \n",
    "        model.add(Dense(512, activation='relu', init='he_uniform'))                                                          \n",
    "        model.add(Dropout(0.5))                                                                                             \n",
    "        model.add(Dense(num_classes, activation='softmax')) # num_classes = 2値分類                                         \n",
    "                                                                                                                            \n",
    "                                                                                                                            \n",
    "        \"\"\"\n",
    "       # load trained model                                                                                                \n",
    "        #json_string = open(os.path.join(f_model, model_filename)).read()\n",
    "        #model = model_from_json(json_string)\n",
    "        #model.load_weights(os.path.join(f_model, weights_filename))\n",
    "        model = load_model('./model/Mymodel2.h5')\n",
    "        \"\"\"                                                                                                                 \n",
    "        model.compile(loss=keras.losses.categorical_crossentropy,                                                           \n",
    "                      optimizer=\"SGD\",                                                                                      \n",
    "                      metrics=['accuracy'])                                                                                 \n",
    "                                                                                                                            \n",
    "        print(model.summary())                                                                                              \n",
    "                                                                                                                            \n",
    "        # callback function                                                                                                 \n",
    "        tb_cb = keras.callbacks.TensorBoard(log_dir=f_log, histogram_freq=1)                                                \n",
    "        cbks = [tb_cb]                                                                                                      \n",
    "                                                                                                                            \n",
    "        # train                                                                                                             \n",
    "        history = model.fit(x_train, y_train,                                                                               \n",
    "                            batch_size=batch_size,                                                                          \n",
    "                            epochs=epochs,                                                                                  \n",
    "                            verbose=1,       #進行状況の表示モード                                                          \n",
    "                            callbacks=cbks,  # [plot_losses, csv_logger],                                                   \n",
    "                            validation_data=(x_test, y_test))                                                               \n",
    "        score_train = model.evaluate(x_train,y_train, verbose=1, batch_size=4)                                              \n",
    "        score_test = model.evaluate(x_test, y_test, verbose=1, batch_size=4)                                                \n",
    "        print('Train loss: {0}'.format(score_train[0]))                                                                     \n",
    "        print('Train accuracy: {0}'.format(score_train[1]))                                                                 \n",
    "        print('Test loss: {0}'.format(score_test[0]))                                                                       \n",
    "        print('Test accuracy: {0}'.format(score_test[1]))                                                                   \n",
    "\n",
    "        model.save('./model/Mymodel2.h5')\n",
    "        \"\"\"\n",
    "        # 学習済みモデル書き出し                                                                                            \n",
    "        json_string = model.to_json()                                                                                       \n",
    "        open(os.path.join(f_model, 'cnn_model.json'), 'w').write(json_string)                                               \n",
    "        yaml_string = model.to_yaml()                                                                                       \n",
    "        open(os.path.join(f_model, 'cnn_model.yaml'), 'w').write(yaml_string)                                               \n",
    "        print('save weights')                                                                                               \n",
    "        model.save_weights(os.path.join(f_model, 'cnn_model_weights.hdf5'))                                                 \n",
    "        \"\"\"\n",
    "       # modelのlayer_nameを調べる                                                                                                                         \n",
    "        for layer in model.layers:\n",
    "            print(layer.name)\n",
    "\n",
    "        layer_name =\"conv2d_4\"# \"max_pooling2d_1\"                                                                                    \n",
    "        intermediate_layer_model = keras.models.Model(inputs=model.input,\n",
    "                                         outputs=model.get_layer(layer_name).output)\n",
    "\n",
    "\n",
    "        layers = model.layers[6:7]     \n",
    "        \n",
    "        urls = [\"/home/seimei/Graduation_Research/dataset_valid/hare/class3-1/image_0104.jpg\", \n",
    "        \"/home/seimei/Graduation_Research/dataset_valid/kumori/class4-5/image_0064.jpg\"]\n",
    "\n",
    "        activations = np.zeros((0,75,75,128))\n",
    "        for url in urls:\n",
    "            img = image.load_img(url, target_size=(300, 300))\n",
    "            img = image.img_to_array(img)\n",
    "            img /= 255\n",
    "            img = np.expand_dims(img, axis=0)\n",
    "            # 指定したlayer_nameと一致するレイヤーの出力を取得する                                                                                              \n",
    "            _activations = intermediate_layer_model.predict(img)\n",
    "            _activations = [activation for layer, activation in zip(layers, _activations) if isinstance(layer, Conv2D)]\n",
    "            print(np.shape(_activations))\n",
    "            activations = np.r_[activations, np.reshape(_activations,(-1,75,75,128))]\n",
    "        print(np.shape(activations))\n",
    "        \"\"\"\n",
    "        # 単品の特徴画像生成#                                                                                                                               \n",
    "        for i, activation in enumerate(activations):\n",
    "            num_of_image = activation.shape[2]\n",
    "            max = np.max(activation[0])\n",
    "            for j in range(0, num_of_image):\n",
    "                plt.figure(figsize=(50, 50))\n",
    "                sns.heatmap(activation[:, :,j], vmin=0, vmax=max, xticklabels=False, yticklabels=False, square=False)\n",
    "                plt.savefig(\"%d_%d.png\" % (i+1, j+1))\n",
    "                plt.close()\n",
    "        \"\"\"\n",
    "        # 出力層ごとに特徴画像を並べてヒートマップ画像として出力                                                                                            \n",
    "        for i, activation in enumerate(activations):\n",
    "            num_of_image = activation.shape[2]\n",
    "            cols = math.ceil(math.sqrt(num_of_image))\n",
    "            rows = math.floor(num_of_image / cols)\n",
    "            screen = []\n",
    "            for y in range(0, rows):\n",
    "                row = []\n",
    "                for x in range(0, cols):\n",
    "                    j = y * cols + x\n",
    "                    if j < num_of_image:\n",
    "                        row.append(activation[:, :, j])\n",
    "                    else:\n",
    "                        row.append(np.zeros())\n",
    "                screen.append(np.concatenate(row, axis=1))\n",
    "            screen = np.concatenate(screen, axis=0)\n",
    "            plt.figure(figsize=(50, 50))\n",
    "            sns.heatmap(screen, xticklabels=False, yticklabels=False)\n",
    "            name = \"maxpooling2d\"\n",
    "            plt.savefig(\"%s.png\" % name)\n",
    "            plt.close()\n",
    "        \n",
    "        y = keras.utils.to_categorical(np.r_[np.zeros(128), np.ones(128)], num_classes)\n",
    "\n",
    "        handle_image_with_pca(activations, y )\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    # add for TeonsorBoard                                                                                                                                  \n",
    "\n",
    "    KTF.set_session(old_session)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conv2D_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(388, 300, 300, 3)\n",
      "(388, 2)\n",
      "conv2d_1\n",
      "conv2d_2\n",
      "max_pooling2d_1\n",
      "dropout_1\n",
      "flatten_1\n",
      "dense_1\n",
      "dropout_2\n",
      "dense_2\n",
      "(1, 296, 296, 64)\n",
      "(1, 296, 296, 64)\n",
      "(2, 296, 296, 64)\n",
      "[[[0.         0.07439406 0.         ... 0.         0.         0.        ]\n",
      "  [0.         0.07530671 0.         ... 0.         0.03418977 0.        ]\n",
      "  [0.         0.01515675 0.         ... 0.         0.00278381 0.        ]\n",
      "  ...\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]]\n",
      "\n",
      " [[0.00895094 0.04726045 0.         ... 0.         0.         0.        ]\n",
      "  [0.00401854 0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  ...\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]]\n",
      "\n",
      " [[0.00656557 0.13469264 0.         ... 0.         0.09424499 0.        ]\n",
      "  [0.00381679 0.11346876 0.         ... 0.         0.08507617 0.        ]\n",
      "  [0.         0.02655922 0.         ... 0.         0.03259697 0.        ]\n",
      "  ...\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.00747126 0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.00395111 0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.00112532 0.         0.         ... 0.         0.         0.        ]\n",
      "  ...\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]]\n",
      "\n",
      " [[0.00505748 0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.01794148 0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.02052829 0.         0.         ... 0.         0.         0.        ]\n",
      "  ...\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]]\n",
      "\n",
      " [[0.02818634 0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.07580794 ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.1568065  ... 0.         0.         0.        ]\n",
      "  ...\n",
      "  [0.         0.         0.         ... 0.         0.03387805 0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]]]\n",
      "['sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa']\n",
      "[[[0.03367632 0.11413164 0.         ... 0.         0.01719323 0.        ]\n",
      "  [0.03367632 0.11413164 0.         ... 0.         0.01719323 0.        ]\n",
      "  [0.03367632 0.11413164 0.         ... 0.         0.01719323 0.        ]\n",
      "  ...\n",
      "  [0.03367632 0.11413164 0.         ... 0.         0.01719323 0.        ]\n",
      "  [0.03367632 0.11413164 0.         ... 0.         0.01719323 0.        ]\n",
      "  [0.03367632 0.11413164 0.         ... 0.         0.01719323 0.        ]]\n",
      "\n",
      " [[0.03367632 0.11413164 0.         ... 0.         0.01719323 0.        ]\n",
      "  [0.03367632 0.11413164 0.         ... 0.         0.01719323 0.        ]\n",
      "  [0.03367632 0.11413164 0.         ... 0.         0.01719323 0.        ]\n",
      "  ...\n",
      "  [0.03367632 0.11413164 0.         ... 0.         0.01719323 0.        ]\n",
      "  [0.03367632 0.11413164 0.         ... 0.         0.01719323 0.        ]\n",
      "  [0.03367632 0.11413164 0.         ... 0.         0.01719323 0.        ]]\n",
      "\n",
      " [[0.03367632 0.11413164 0.         ... 0.         0.01719323 0.        ]\n",
      "  [0.03367632 0.11413164 0.         ... 0.         0.01719323 0.        ]\n",
      "  [0.03367632 0.11413164 0.         ... 0.         0.01719323 0.        ]\n",
      "  ...\n",
      "  [0.03367632 0.11413164 0.         ... 0.         0.01719323 0.        ]\n",
      "  [0.03367632 0.11413164 0.         ... 0.         0.01719323 0.        ]\n",
      "  [0.03367632 0.11413164 0.         ... 0.         0.01719323 0.        ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  ...\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]]\n",
      "\n",
      " [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  ...\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]]\n",
      "\n",
      " [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  ...\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]]]\n",
      "['cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa']\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa9c459b0b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "config = tf.ConfigProto(device_count={'GPU':1,'CPU':56})\n",
    "sess = tf.Session(config=config)\n",
    "keras.backend.set_session(sess)\n",
    "\n",
    "STANDARD_SIZE = (300, 300)\n",
    "\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "np.random.seed(0)\n",
    "rn.seed(0)\n",
    "\n",
    "#session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)                              \n",
    "tf.set_random_seed(0)\n",
    "\n",
    "input_shape = (300, 300, 3)\n",
    "batch_size = 4\n",
    "epochs = 10\n",
    "num_classes = 2\n",
    "\n",
    "f_log = './logs'\n",
    "f_model = './model'\n",
    "\n",
    "\n",
    "model_filename = 'cnn_model.json'\n",
    "weights_filename = 'cnn_model_weights.hdf5'\n",
    "\n",
    "def input_data(path_train, path_test):\n",
    "\n",
    "    x = []\n",
    "\n",
    "    with open(path_train, \"r\") as f:\n",
    "        train_path_list = f.readlines()\n",
    "\n",
    "    filenames = []\n",
    "    labels = []\n",
    "    for row in train_path_list:\n",
    "        row = row.split(\" \")\n",
    "        filenames.append(row[0])\n",
    "        labels.append(row[1])\n",
    "\n",
    "    label = []\n",
    "    for i in labels:\n",
    "        label.append(int(i))\n",
    "\n",
    "    for f in filenames:\n",
    "        x.append(image.img_to_array(img_to_matrix(f)))\n",
    "\n",
    "    x = np.asarray(x)\n",
    "    #正規化                                                                                                                 \n",
    "    x /= 255\n",
    "    y = np.asarray(label)\n",
    "    y = keras.utils.to_categorical(y, num_classes)\n",
    "\n",
    "    '==============================================================='\n",
    "\n",
    "    a = []\n",
    "    \n",
    "    with open(path_test, \"r\") as f:\n",
    "        train_path_list = f.readlines()\n",
    "\n",
    "    filenames1 = []\n",
    "    labels1 = []\n",
    "    for row in train_path_list:\n",
    "        row = row.split(\" \")\n",
    "        filenames1.append(row[0])\n",
    "        labels1.append(row[1])\n",
    "\n",
    "    label1 = []\n",
    "    for i in labels1:\n",
    "        label1.append(int(i))\n",
    "\n",
    "    for f in filenames1:\n",
    "        a.append(image.img_to_array(img_to_matrix(f)))\n",
    "\n",
    "    test_data = np.asarray(a)\n",
    "    # 正規化                                                                                                                \n",
    "    test_data /= 255\n",
    "    test_label = np.asarray(label1)\n",
    "    test_label = keras.utils.to_categorical(test_label, num_classes)\n",
    "\n",
    "    train_data, valid_data, train_label, valid_label = cross_validation.train_test_split(x, y, test_size=0.3)\n",
    "    test_data, valid1, test_label, valid2 = cross_validation.train_test_split(test_data, test_label, test_size=0.3)\n",
    "    \n",
    "    print(np.shape(train_data))\n",
    "    print(np.shape(train_label))\n",
    "    return train_data, test_data, train_label, test_label, valid_data, valid_label\n",
    "\n",
    "\n",
    "\n",
    "# parse image                                                                                                               \n",
    "def img_to_matrix(filename, verbose=False):\n",
    "    img = Image.open(filename)\n",
    "    if verbose:\n",
    "        print('changing size from %s to %s' % (str(img.size), str(STANDARD_SIZE)))\n",
    "    img = img.resize(STANDARD_SIZE)\n",
    "    imgArray = np.asarray(img)\n",
    "    return imgArray  # imgArray.shape = (167 x 300 x 3)                                                                     \n",
    "\n",
    "\n",
    "# 1次元に引き延ばす(PCAで使用)                                                                                              \n",
    "def flatten_image(img):\n",
    "\n",
    "    s = img.shape[0] * img.shape[1] #* img.shape[2]\n",
    "    img_wide = img.reshape(1, s)\n",
    "    return img_wide[0]\n",
    "\n",
    "\n",
    "def handle_image_with_pca(activations, y_test):\n",
    "\n",
    "    for i in range(2):\n",
    "        images = activations[i,:,:,:]\n",
    "        print(images)\n",
    "        if i == 0:\n",
    "            labels = y_test[:64]\n",
    "        elif i == 1:\n",
    "            labels = y_test[64:]\n",
    "        ls = []\n",
    "\n",
    "        for label in labels:\n",
    "\n",
    "            if list(label) == [0,1]:\n",
    "                ls.append(\"cloudy_seesaa\")\n",
    "            elif list(label) == [1,0]:\n",
    "                ls.append(\"sunny_seesaa\")\n",
    "        labels = ls\n",
    "        print(labels)\n",
    "        data = []\n",
    "        for a in range(64):#for image in images:\n",
    "            img = flatten_image(images[:, :, a])\n",
    "            data.append(img)\n",
    "\n",
    "        data = np.array(data)\n",
    "\n",
    "        is_train = np.random.uniform(0, 1, len(data)) <= 0.7\n",
    "        y = np.where(np.array(labels) == 'cloudy_seesaa', 1, 0)\n",
    "\n",
    "        train_x, train_y = data[is_train], y[is_train]\n",
    "\n",
    "        pca = PCA(n_components=2)\n",
    "        X = pca.fit_transform(data)\n",
    "        if i == 0:\n",
    "            df1 = pd.DataFrame({\"x\": X[:, 0], \"y\": X[:, 1],\n",
    "                               \"label\": np.where(y == 1, 'cloudy_seesaa', 'sunny_seesaa')})\n",
    "        elif i == 1:\n",
    "            df2 = pd.DataFrame({\"x\": X[:, 0], \"y\": X[:, 1],\n",
    "                               \"label\": np.where(y == 1, 'cloudy_seesaa', 'sunny_seesaa')})\n",
    "            df = pd.concat([df1, df2])\n",
    "    colors = ['red','blue']\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for label, color in zip(df['label'].unique(), colors):\n",
    "        mask = df['label'] == label\n",
    "        plt.scatter(df[mask]['x'], df[mask]['y'], c=color, label=label)\n",
    "    sns.set()\n",
    "    plt.xlabel(\"pc1 (Principal Component1)\")  # 全データの分散が最大となる方向                                              \n",
    "    plt.ylabel(\"pc2 (Principal Component2)\")  # 第一主成分に垂直な方向の軸                                                  \n",
    "    plt.legend()\n",
    "    #plt.show()                                                                                                             \n",
    "    plt.savefig('pca_feature1.png')\n",
    "\n",
    "    # training a classifier                                                                                                 \n",
    "    pca = PCA(n_components=2)\n",
    "    train_x = pca.fit_transform(train_x)\n",
    "    \"\"\"\n",
    "    svm = LinearSVC(C=1.0)\n",
    "    svm.fit(train_x, train_y)\n",
    "    joblib.dump(svm, 'model.pkl')\n",
    "\n",
    "    # evaluating the model                                                                                                  \n",
    "    test_x, test_y = data[is_train == False], y[is_train == False]\n",
    "    test_x = pca.transform(test_x)\n",
    "    print(pd.crosstab(test_y, svm.predict(test_x),\n",
    "                      rownames=['Actual'], colnames=['Predicted']))\n",
    "    \"\"\"\n",
    "def main():\n",
    "\n",
    "    x_train, x_test, y_train, y_test, valid_data, valid_label = input_data(\"path_and_label_train.txt\",\n",
    "                                                                           \"path_and_label_test.txt\")\n",
    "\n",
    "    old_session = KTF.get_session()\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        session = tf.Session('')\n",
    "        KTF.set_session(session)\n",
    "        KTF.set_learning_phase(1)\n",
    "        \"\"\"\n",
    "        model = Sequential()                                                                                                \n",
    "        model.add(Conv2D(32, kernel_size=(3, 3),                                                                            \n",
    "                         activation='relu',                                                                                 \n",
    "                         input_shape=input_shape, kernel_initializer=\"he_normal\",                        \n",
    "                         bias_initializer=\"zeros\"))                                                                         \n",
    "        model.add(Conv2D(64, (3, 3), activation='relu'))                                                                    \n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Dropout(0.25))                                                                                            \n",
    "        model.add(Flatten())                                                                                                \n",
    "        model.add(Dense(28, activation='relu', init='he_uniform'))                                                          \n",
    "        model.add(Dropout(0.5))                                                                                             \n",
    "        model.add(Dense(num_classes, activation='softmax')) # num_classes = 2値分類                                         \n",
    "                                                                                                                            \n",
    "                                                                                                                            \n",
    "        \"\"\"\n",
    "       # load trained model                                                                                                \n",
    "        json_string = open(os.path.join(f_model, model_filename)).read()\n",
    "        model = model_from_json(json_string)\n",
    "        model.load_weights(os.path.join(f_model, weights_filename))\n",
    "        \"\"\"                                                                                                                 \n",
    "        model.compile(loss=keras.losses.categorical_crossentropy,                                                           \n",
    "                      optimizer=\"SGD\",                                                                                      \n",
    "                      metrics=['accuracy'])                                                                                 \n",
    "                                                                                                                            \n",
    "        print(model.summary())                                                                                              \n",
    "                                                                                                                            \n",
    "        # callback function                                                                                                 \n",
    "        tb_cb = keras.callbacks.TensorBoard(log_dir=f_log, histogram_freq=1)                                                \n",
    "        cbks = [tb_cb]                                                                                                      \n",
    "                                                                                                                            \n",
    "        # train                                                                                                             \n",
    "        history = model.fit(x_train, y_train,                                                                               \n",
    "                            batch_size=batch_size,                                                                          \n",
    "                            epochs=epochs,                                                                                  \n",
    "                            verbose=1,       #進行状況の表示モード                                                          \n",
    "                            callbacks=cbks,  # [plot_losses, csv_logger],                                                   \n",
    "                            validation_data=(x_test, y_test))                                                               \n",
    "        score_train = model.evaluate(x_train,y_train, verbose=1, batch_size=4)                                              \n",
    "        score_test = model.evaluate(x_test, y_test, verbose=1, batch_size=4)                                                \n",
    "        print('Train loss: {0}'.format(score_train[0]))                                                                     \n",
    "        print('Train accuracy: {0}'.format(score_train[1]))                                                                 \n",
    "        print('Test loss: {0}'.format(score_test[0]))                                                                       \n",
    "        print('Test accuracy: {0}'.format(score_test[1]))                                                                   \n",
    "                                                                                                                            \n",
    "        # 学習済みモデル書き出し                                                                                            \n",
    "        json_string = model.to_json()                                                                                       \n",
    "        open(os.path.join(f_model, 'cnn_model.json'), 'w').write(json_string)                                               \n",
    "        yaml_string = model.to_yaml()                                                                                       \n",
    "        open(os.path.join(f_model, 'cnn_model.yaml'), 'w').write(yaml_string)                                               \n",
    "        print('save weights')                                                                                               \n",
    "        model.save_weights(os.path.join(f_model, 'cnn_model_weights.hdf5'))                                                 \n",
    "        \"\"\"\n",
    "       # modelのlayer_nameを調べる                                                                                                                         \n",
    "        for layer in model.layers:\n",
    "            print(layer.name)\n",
    "\n",
    "        layer_name =\"conv2d_2\"# \"max_pooling2d_1\"                                                                                    \n",
    "        intermediate_layer_model = keras.models.Model(inputs=model.input,\n",
    "                                         outputs=model.get_layer(layer_name).output)\n",
    "\n",
    "\n",
    "        layers = model.layers[1:2]     \n",
    "        \n",
    "        urls = [\"/home/seimei/Graduation_Research/dataset_valid/hare/class3-1/image_0104.jpg\", \n",
    "        \"/home/seimei/Graduation_Research/dataset_valid/kumori/class4-5/image_0064.jpg\"]\n",
    "\n",
    "        activations = np.zeros((0,296,296,64))\n",
    "        for url in urls:\n",
    "            img = image.load_img(url, target_size=(300, 300))\n",
    "            img = image.img_to_array(img)\n",
    "            img /= 255\n",
    "            img = np.expand_dims(img, axis=0)\n",
    "            # 指定したlayer_nameと一致するレイヤーの出力を取得する                                                                                              \n",
    "            _activations = intermediate_layer_model.predict(img)\n",
    "            _activations = [activation for layer, activation in zip(layers, _activations) if isinstance(layer, Conv2D)]\n",
    "            print(np.shape(_activations))\n",
    "            activations = np.r_[activations, np.reshape(_activations,(-1,296,296,64))]\n",
    "        print(np.shape(activations))\n",
    "        \"\"\"\n",
    "        # 単品の特徴画像生成#                                                                                                                               \n",
    "        for i, activation in enumerate(activations):\n",
    "            num_of_image = activation.shape[2]\n",
    "            max = np.max(activation[0])\n",
    "            for j in range(0, num_of_image):\n",
    "                plt.figure(figsize=(50, 50))\n",
    "                sns.heatmap(activation[:, :,j], vmin=0, vmax=max, xticklabels=False, yticklabels=False, square=False)\n",
    "                plt.savefig(\"%d_%d.png\" % (i+1, j+1))\n",
    "                plt.close()\n",
    "        # 出力層ごとに特徴画像を並べてヒートマップ画像として出力                                                                                            \n",
    "        for i, activation in enumerate(activations):\n",
    "            num_of_image = activation.shape[2]\n",
    "            cols = math.ceil(math.sqrt(num_of_image))\n",
    "            rows = math.floor(num_of_image / cols)\n",
    "            screen = []\n",
    "            for y in range(0, rows):\n",
    "                row = []\n",
    "                for x in range(0, cols):\n",
    "                    j = y * cols + x\n",
    "                    if j < num_of_image:\n",
    "                        row.append(activation[:, :, j])\n",
    "                    else:\n",
    "                        row.append(np.zeros())\n",
    "                screen.append(np.concatenate(row, axis=1))\n",
    "            screen = np.concatenate(screen, axis=0)\n",
    "            plt.figure(figsize=(50, 50))\n",
    "            sns.heatmap(screen, xticklabels=False, yticklabels=False)\n",
    "            name = \"maxpooling2d\"\n",
    "            plt.savefig(\"%s.png\" % name)\n",
    "            plt.close()\n",
    "        \"\"\"\n",
    "        y = keras.utils.to_categorical(np.r_[np.zeros(64), np.ones(64)], num_classes)\n",
    "\n",
    "        handle_image_with_pca(activations, y )\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    # add for TeonsorBoard                                                                                                                                  \n",
    "\n",
    "    KTF.set_session(old_session)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(388, 300, 300, 3)\n",
      "(388, 2)\n",
      "conv2d_1\n",
      "conv2d_2\n",
      "max_pooling2d_1\n",
      "dropout_1\n",
      "flatten_1\n",
      "dense_1\n",
      "dropout_2\n",
      "dense_2\n",
      "(1, 148, 148, 64)\n",
      "(1, 148, 148, 64)\n",
      "(2, 148, 148, 64)\n",
      "[[[8.95094033e-03 7.53067061e-02 0.00000000e+00 ... 0.00000000e+00\n",
      "   3.41897681e-02 0.00000000e+00]\n",
      "  [0.00000000e+00 1.66509151e-02 0.00000000e+00 ... 0.00000000e+00\n",
      "   2.78380886e-03 0.00000000e+00]\n",
      "  [1.69676319e-02 1.10873282e-02 0.00000000e+00 ... 0.00000000e+00\n",
      "   6.21160269e-02 0.00000000e+00]\n",
      "  ...\n",
      "  [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "   1.29721873e-03 0.00000000e+00]\n",
      "  [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00]\n",
      "  [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00]]\n",
      "\n",
      " [[2.30023172e-02 1.62715048e-01 0.00000000e+00 ... 0.00000000e+00\n",
      "   9.42449942e-02 0.00000000e+00]\n",
      "  [0.00000000e+00 1.65592134e-01 0.00000000e+00 ... 0.00000000e+00\n",
      "   6.75950274e-02 0.00000000e+00]\n",
      "  [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00]\n",
      "  ...\n",
      "  [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00]\n",
      "  [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00]\n",
      "  [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00]]\n",
      "\n",
      " [[3.09417658e-02 7.20343888e-02 0.00000000e+00 ... 5.93928620e-04\n",
      "   2.69216485e-03 0.00000000e+00]\n",
      "  [2.89761722e-02 1.43354028e-01 0.00000000e+00 ... 4.67823632e-03\n",
      "   3.83617133e-02 0.00000000e+00]\n",
      "  [2.73942947e-03 8.21036994e-02 0.00000000e+00 ... 0.00000000e+00\n",
      "   4.15758416e-02 0.00000000e+00]\n",
      "  ...\n",
      "  [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00]\n",
      "  [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00]\n",
      "  [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[2.59929337e-03 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00]\n",
      "  [2.13361904e-03 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00]\n",
      "  [3.36337835e-05 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00]\n",
      "  ...\n",
      "  [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "   8.30965675e-03 0.00000000e+00]\n",
      "  [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00]\n",
      "  [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00]]\n",
      "\n",
      " [[7.47125782e-03 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "   2.40569375e-03 0.00000000e+00]\n",
      "  [1.23320529e-02 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00]\n",
      "  [1.09398589e-02 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00]\n",
      "  ...\n",
      "  [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00]\n",
      "  [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00]\n",
      "  [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00]]\n",
      "\n",
      " [[2.81863399e-02 0.00000000e+00 7.58079365e-02 ... 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00]\n",
      "  [2.05282867e-02 0.00000000e+00 2.17877597e-01 ... 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00]\n",
      "  [0.00000000e+00 0.00000000e+00 3.25989544e-01 ... 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00]\n",
      "  ...\n",
      "  [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "   2.17684340e-02 0.00000000e+00]\n",
      "  [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "   3.38780507e-02 0.00000000e+00]\n",
      "  [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00]]]\n",
      "['sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa', 'sunny_seesaa']\n",
      "[[[0.03367632 0.11413164 0.         ... 0.         0.01719323 0.        ]\n",
      "  [0.03367632 0.11413164 0.         ... 0.         0.01719323 0.        ]\n",
      "  [0.03367632 0.11413164 0.         ... 0.         0.01719323 0.        ]\n",
      "  ...\n",
      "  [0.03367632 0.11413164 0.         ... 0.         0.01719323 0.        ]\n",
      "  [0.03367632 0.11413164 0.         ... 0.         0.01719323 0.        ]\n",
      "  [0.03367632 0.11413164 0.         ... 0.         0.01719323 0.        ]]\n",
      "\n",
      " [[0.03367632 0.11413164 0.         ... 0.         0.01719323 0.        ]\n",
      "  [0.03367632 0.11413164 0.         ... 0.         0.01719323 0.        ]\n",
      "  [0.03367632 0.11413164 0.         ... 0.         0.01719323 0.        ]\n",
      "  ...\n",
      "  [0.03367632 0.11413164 0.         ... 0.         0.01719323 0.        ]\n",
      "  [0.03367632 0.11413164 0.         ... 0.         0.01719323 0.        ]\n",
      "  [0.03367632 0.11413164 0.         ... 0.         0.01719323 0.        ]]\n",
      "\n",
      " [[0.03367632 0.11413164 0.55462241 ... 0.         0.01719323 0.        ]\n",
      "  [0.03367632 0.11413164 0.53849775 ... 0.         0.01719323 0.        ]\n",
      "  [0.03367632 0.11413164 0.53801423 ... 0.         0.01719323 0.        ]\n",
      "  ...\n",
      "  [0.04142198 0.11413164 0.         ... 0.         0.01719323 0.        ]\n",
      "  [0.05152429 0.11413164 0.         ... 0.         0.01719323 0.        ]\n",
      "  [0.04316497 0.11413164 0.         ... 0.         0.01719323 0.        ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.         0.         0.         ... 0.         0.00139258 0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.05089359 0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  ...\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]]\n",
      "\n",
      " [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  ...\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]]\n",
      "\n",
      " [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  ...\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]]]\n",
      "['cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa', 'cloudy_seesaa']\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa9dcecaef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "config = tf.ConfigProto(device_count={'GPU':1,'CPU':56})\n",
    "sess = tf.Session(config=config)\n",
    "keras.backend.set_session(sess)\n",
    "\n",
    "STANDARD_SIZE = (300, 300)\n",
    "\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "np.random.seed(0)\n",
    "rn.seed(0)\n",
    "\n",
    "#session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)                              \n",
    "tf.set_random_seed(0)\n",
    "\n",
    "input_shape = (300, 300, 3)\n",
    "batch_size = 4\n",
    "epochs = 10\n",
    "num_classes = 2\n",
    "\n",
    "f_log = './logs'\n",
    "f_model = './model'\n",
    "\n",
    "\n",
    "model_filename = 'cnn_model.json'\n",
    "weights_filename = 'cnn_model_weights.hdf5'\n",
    "\n",
    "def input_data(path_train, path_test):\n",
    "\n",
    "    x = []\n",
    "\n",
    "    with open(path_train, \"r\") as f:\n",
    "        train_path_list = f.readlines()\n",
    "\n",
    "    filenames = []\n",
    "    labels = []\n",
    "    for row in train_path_list:\n",
    "        row = row.split(\" \")\n",
    "        filenames.append(row[0])\n",
    "        labels.append(row[1])\n",
    "\n",
    "    label = []\n",
    "    for i in labels:\n",
    "        label.append(int(i))\n",
    "\n",
    "    for f in filenames:\n",
    "        x.append(image.img_to_array(img_to_matrix(f)))\n",
    "\n",
    "    x = np.asarray(x)\n",
    "    #正規化                                                                                                                 \n",
    "    x /= 255\n",
    "    y = np.asarray(label)\n",
    "    y = keras.utils.to_categorical(y, num_classes)\n",
    "\n",
    "    '==============================================================='\n",
    "\n",
    "    a = []\n",
    "    \n",
    "    with open(path_test, \"r\") as f:\n",
    "        train_path_list = f.readlines()\n",
    "\n",
    "    filenames1 = []\n",
    "    labels1 = []\n",
    "    for row in train_path_list:\n",
    "        row = row.split(\" \")\n",
    "        filenames1.append(row[0])\n",
    "        labels1.append(row[1])\n",
    "\n",
    "    label1 = []\n",
    "    for i in labels1:\n",
    "        label1.append(int(i))\n",
    "\n",
    "    for f in filenames1:\n",
    "        a.append(image.img_to_array(img_to_matrix(f)))\n",
    "\n",
    "    test_data = np.asarray(a)\n",
    "    # 正規化                                                                                                                \n",
    "    test_data /= 255\n",
    "    test_label = np.asarray(label1)\n",
    "    test_label = keras.utils.to_categorical(test_label, num_classes)\n",
    "\n",
    "    train_data, valid_data, train_label, valid_label = cross_validation.train_test_split(x, y, test_size=0.3)\n",
    "    test_data, valid1, test_label, valid2 = cross_validation.train_test_split(test_data, test_label, test_size=0.3)\n",
    "    \n",
    "    print(np.shape(train_data))\n",
    "    print(np.shape(train_label))\n",
    "    return train_data, test_data, train_label, test_label, valid_data, valid_label\n",
    "\n",
    "\n",
    "\n",
    "# parse image                                                                                                               \n",
    "def img_to_matrix(filename, verbose=False):\n",
    "    img = Image.open(filename)\n",
    "    if verbose:\n",
    "        print('changing size from %s to %s' % (str(img.size), str(STANDARD_SIZE)))\n",
    "    img = img.resize(STANDARD_SIZE)\n",
    "    imgArray = np.asarray(img)\n",
    "    return imgArray  # imgArray.shape = (167 x 300 x 3)                                                                     \n",
    "\n",
    "\n",
    "# 1次元に引き延ばす(PCAで使用)                                                                                              \n",
    "def flatten_image(img):\n",
    "\n",
    "    s = img.shape[0] * img.shape[1] #* img.shape[2]\n",
    "    img_wide = img.reshape(1, s)\n",
    "    return img_wide[0]\n",
    "\n",
    "\n",
    "def handle_image_with_pca(activations, y_test):\n",
    "\n",
    "    for i in range(2):\n",
    "        images = activations[i,:,:,:]\n",
    "        print(images)\n",
    "        if i == 0:\n",
    "            labels = y_test[:64]\n",
    "        elif i == 1:\n",
    "            labels = y_test[64:]\n",
    "        ls = []\n",
    "\n",
    "        for label in labels:\n",
    "\n",
    "            if list(label) == [0,1]:\n",
    "                ls.append(\"cloudy_seesaa\")\n",
    "            elif list(label) == [1,0]:\n",
    "                ls.append(\"sunny_seesaa\")\n",
    "        labels = ls\n",
    "        print(labels)\n",
    "        data = []\n",
    "        for a in range(64):#for image in images:\n",
    "            img = flatten_image(images[:, :, a])\n",
    "            data.append(img)\n",
    "\n",
    "        data = np.array(data)\n",
    "\n",
    "        is_train = np.random.uniform(0, 1, len(data)) <= 0.7\n",
    "        y = np.where(np.array(labels) == 'cloudy_seesaa', 1, 0)\n",
    "\n",
    "        train_x, train_y = data[is_train], y[is_train]\n",
    "\n",
    "        pca = PCA(n_components=2)\n",
    "        X = pca.fit_transform(data)\n",
    "        if i == 0:\n",
    "            df1 = pd.DataFrame({\"x\": X[:, 0], \"y\": X[:, 1],\n",
    "                               \"label\": np.where(y == 1, 'cloudy_seesaa', 'sunny_seesaa')})\n",
    "        elif i == 1:\n",
    "            df2 = pd.DataFrame({\"x\": X[:, 0], \"y\": X[:, 1],\n",
    "                               \"label\": np.where(y == 1, 'cloudy_seesaa', 'sunny_seesaa')})\n",
    "            df = pd.concat([df1, df2])\n",
    "    colors = ['red','blue']\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for label, color in zip(df['label'].unique(), colors):\n",
    "        mask = df['label'] == label\n",
    "        plt.scatter(df[mask]['x'], df[mask]['y'], c=color, label=label)\n",
    "    sns.set()\n",
    "    plt.xlabel(\"pc1 (Principal Component1)\")  # 全データの分散が最大となる方向                                              \n",
    "    plt.ylabel(\"pc2 (Principal Component2)\")  # 第一主成分に垂直な方向の軸                                                  \n",
    "    plt.legend()\n",
    "    #plt.show()                                                                                                             \n",
    "    plt.savefig('pca_feature1.png')\n",
    "\n",
    "    # training a classifier                                                                                                 \n",
    "    pca = PCA(n_components=2)\n",
    "    train_x = pca.fit_transform(train_x)\n",
    "    \"\"\"\n",
    "    svm = LinearSVC(C=1.0)\n",
    "    svm.fit(train_x, train_y)\n",
    "    joblib.dump(svm, 'model.pkl')\n",
    "\n",
    "    # evaluating the model                                                                                                  \n",
    "    test_x, test_y = data[is_train == False], y[is_train == False]\n",
    "    test_x = pca.transform(test_x)\n",
    "    print(pd.crosstab(test_y, svm.predict(test_x),\n",
    "                      rownames=['Actual'], colnames=['Predicted']))\n",
    "    \"\"\"\n",
    "def main():\n",
    "\n",
    "    x_train, x_test, y_train, y_test, valid_data, valid_label = input_data(\"path_and_label_train.txt\",\n",
    "                                                                           \"path_and_label_test.txt\")\n",
    "\n",
    "    old_session = KTF.get_session()\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        session = tf.Session('')\n",
    "        KTF.set_session(session)\n",
    "        KTF.set_learning_phase(1)\n",
    "        \"\"\"\n",
    "        model = Sequential()                                                                                                \n",
    "        model.add(Conv2D(32, kernel_size=(3, 3),                                                                            \n",
    "                         activation='relu',                                                                                 \n",
    "                         input_shape=input_shape, kernel_initializer=\"he_normal\",                        \n",
    "                         bias_initializer=\"zeros\"))                                                                         \n",
    "        model.add(Conv2D(64, (3, 3), activation='relu'))                                                                    \n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Dropout(0.25))                                                                                            \n",
    "        model.add(Flatten())                                                                                                \n",
    "        model.add(Dense(28, activation='relu', init='he_uniform'))                                                          \n",
    "        model.add(Dropout(0.5))                                                                                             \n",
    "        model.add(Dense(num_classes, activation='softmax')) # num_classes = 2値分類                                         \n",
    "                                                                                                                            \n",
    "                                                                                                                            \n",
    "        \"\"\"\n",
    "       # load trained model                                                                                                \n",
    "        json_string = open(os.path.join(f_model, model_filename)).read()\n",
    "        model = model_from_json(json_string)\n",
    "        model.load_weights(os.path.join(f_model, weights_filename))\n",
    "        \"\"\"                                                                                                                 \n",
    "        model.compile(loss=keras.losses.categorical_crossentropy,                                                           \n",
    "                      optimizer=\"SGD\",                                                                                      \n",
    "                      metrics=['accuracy'])                                                                                 \n",
    "                                                                                                                            \n",
    "        print(model.summary())                                                                                              \n",
    "                                                                                                                            \n",
    "        # callback function                                                                                                 \n",
    "        tb_cb = keras.callbacks.TensorBoard(log_dir=f_log, histogram_freq=1)                                                \n",
    "        cbks = [tb_cb]                                                                                                      \n",
    "                                                                                                                            \n",
    "        # train                                                                                                             \n",
    "        history = model.fit(x_train, y_train,                                                                               \n",
    "                            batch_size=batch_size,                                                                          \n",
    "                            epochs=epochs,                                                                                  \n",
    "                            verbose=1,       #進行状況の表示モード                                                          \n",
    "                            callbacks=cbks,  # [plot_losses, csv_logger],                                                   \n",
    "                            validation_data=(x_test, y_test))                                                               \n",
    "        score_train = model.evaluate(x_train,y_train, verbose=1, batch_size=4)                                              \n",
    "        score_test = model.evaluate(x_test, y_test, verbose=1, batch_size=4)                                                \n",
    "        print('Train loss: {0}'.format(score_train[0]))                                                                     \n",
    "        print('Train accuracy: {0}'.format(score_train[1]))                                                                 \n",
    "        print('Test loss: {0}'.format(score_test[0]))                                                                       \n",
    "        print('Test accuracy: {0}'.format(score_test[1]))                                                                   \n",
    "                                                                                                                            \n",
    "        # 学習済みモデル書き出し                                                                                            \n",
    "        json_string = model.to_json()                                                                                       \n",
    "        open(os.path.join(f_model, 'cnn_model.json'), 'w').write(json_string)                                               \n",
    "        yaml_string = model.to_yaml()                                                                                       \n",
    "        open(os.path.join(f_model, 'cnn_model.yaml'), 'w').write(yaml_string)                                               \n",
    "        print('save weights')                                                                                               \n",
    "        model.save_weights(os.path.join(f_model, 'cnn_model_weights.hdf5'))                                                 \n",
    "        \"\"\"\n",
    "       # modelのlayer_nameを調べる                                                                                                                         \n",
    "        for layer in model.layers:\n",
    "            print(layer.name)\n",
    "\n",
    "        layer_name =\"max_pooling2d_1\"#\"conv2d_2\"# \"max_pooling2d_1\"                                                                                    \n",
    "        intermediate_layer_model = keras.models.Model(inputs=model.input,\n",
    "                                         outputs=model.get_layer(layer_name).output)\n",
    "\n",
    "\n",
    "        layers = model.layers[2:3]     \n",
    "        \n",
    "        urls = [\"/home/seimei/Graduation_Research/dataset_valid/hare/class3-1/image_0104.jpg\", \n",
    "        \"/home/seimei/Graduation_Research/dataset_valid/kumori/class4-5/image_0064.jpg\"]\n",
    "\n",
    "        activations = np.zeros((0,148,148,64))\n",
    "        for url in urls:\n",
    "            img = image.load_img(url, target_size=(300, 300))\n",
    "            img = image.img_to_array(img)\n",
    "            img /= 255\n",
    "            img = np.expand_dims(img, axis=0)\n",
    "            # 指定したlayer_nameと一致するレイヤーの出力を取得する                                                                                              \n",
    "            _activations = intermediate_layer_model.predict(img)\n",
    "            _activations = [activation for layer, activation in zip(layers, _activations) if isinstance(layer, MaxPooling2D)]\n",
    "            print(np.shape(_activations))\n",
    "            activations = np.r_[activations, np.reshape(_activations,(-1,148,148,64))]\n",
    "        print(np.shape(activations))\n",
    "        \"\"\"\n",
    "        # 単品の特徴画像生成#                                                                                                                               \n",
    "        for i, activation in enumerate(activations):\n",
    "            num_of_image = activation.shape[2]\n",
    "            max = np.max(activation[0])\n",
    "            for j in range(0, num_of_image):\n",
    "                plt.figure(figsize=(50, 50))\n",
    "                sns.heatmap(activation[:, :,j], vmin=0, vmax=max, xticklabels=False, yticklabels=False, square=False)\n",
    "                plt.savefig(\"%d_%d.png\" % (i+1, j+1))\n",
    "                plt.close()\n",
    "        # 出力層ごとに特徴画像を並べてヒートマップ画像として出力                                                                                            \n",
    "        for i, activation in enumerate(activations):\n",
    "            num_of_image = activation.shape[2]\n",
    "            cols = math.ceil(math.sqrt(num_of_image))\n",
    "            rows = math.floor(num_of_image / cols)\n",
    "            screen = []\n",
    "            for y in range(0, rows):\n",
    "                row = []\n",
    "                for x in range(0, cols):\n",
    "                    j = y * cols + x\n",
    "                    if j < num_of_image:\n",
    "                        row.append(activation[:, :, j])\n",
    "                    else:\n",
    "                        row.append(np.zeros())\n",
    "                screen.append(np.concatenate(row, axis=1))\n",
    "            screen = np.concatenate(screen, axis=0)\n",
    "            plt.figure(figsize=(50, 50))\n",
    "            sns.heatmap(screen, xticklabels=False, yticklabels=False)\n",
    "            name = \"maxpooling2d\"\n",
    "            plt.savefig(\"%s.png\" % name)\n",
    "            plt.close()\n",
    "        \"\"\"\n",
    "        y = keras.utils.to_categorical(np.r_[np.zeros(64), np.ones(64)], num_classes)\n",
    "\n",
    "        handle_image_with_pca(activations, y )\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    # add for TeonsorBoard                                                                                                                                  \n",
    "\n",
    "    KTF.set_session(old_session)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DropOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "config = tf.ConfigProto(device_count={'GPU':1,'CPU':56})\n",
    "sess = tf.Session(config=config)\n",
    "keras.backend.set_session(sess)\n",
    "\n",
    "STANDARD_SIZE = (300, 300)\n",
    "\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "np.random.seed(0)\n",
    "rn.seed(0)\n",
    "\n",
    "#session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)                              \n",
    "tf.set_random_seed(0)\n",
    "\n",
    "input_shape = (300, 300, 3)\n",
    "batch_size = 4\n",
    "epochs = 10\n",
    "num_classes = 2\n",
    "\n",
    "f_log = './logs'\n",
    "f_model = './model'\n",
    "\n",
    "\n",
    "model_filename = 'cnn_model.json'\n",
    "weights_filename = 'cnn_model_weights.hdf5'\n",
    "\n",
    "def input_data(path_train, path_test):\n",
    "\n",
    "    x = []\n",
    "\n",
    "    with open(path_train, \"r\") as f:\n",
    "        train_path_list = f.readlines()\n",
    "\n",
    "    filenames = []\n",
    "    labels = []\n",
    "    for row in train_path_list:\n",
    "        row = row.split(\" \")\n",
    "        filenames.append(row[0])\n",
    "        labels.append(row[1])\n",
    "\n",
    "    label = []\n",
    "    for i in labels:\n",
    "        label.append(int(i))\n",
    "\n",
    "    for f in filenames:\n",
    "        x.append(image.img_to_array(img_to_matrix(f)))\n",
    "\n",
    "    x = np.asarray(x)\n",
    "    #正規化                                                                                                                 \n",
    "    x /= 255\n",
    "    y = np.asarray(label)\n",
    "    y = keras.utils.to_categorical(y, num_classes)\n",
    "\n",
    "    '==============================================================='\n",
    "\n",
    "    a = []\n",
    "    \n",
    "    with open(path_test, \"r\") as f:\n",
    "        train_path_list = f.readlines()\n",
    "\n",
    "    filenames1 = []\n",
    "    labels1 = []\n",
    "    for row in train_path_list:\n",
    "        row = row.split(\" \")\n",
    "        filenames1.append(row[0])\n",
    "        labels1.append(row[1])\n",
    "\n",
    "    label1 = []\n",
    "    for i in labels1:\n",
    "        label1.append(int(i))\n",
    "\n",
    "    for f in filenames1:\n",
    "        a.append(image.img_to_array(img_to_matrix(f)))\n",
    "\n",
    "    test_data = np.asarray(a)\n",
    "    # 正規化                                                                                                                \n",
    "    test_data /= 255\n",
    "    test_label = np.asarray(label1)\n",
    "    test_label = keras.utils.to_categorical(test_label, num_classes)\n",
    "\n",
    "    train_data, valid_data, train_label, valid_label = cross_validation.train_test_split(x, y, test_size=0.3)\n",
    "    test_data, valid1, test_label, valid2 = cross_validation.train_test_split(test_data, test_label, test_size=0.3)\n",
    "    \n",
    "    print(np.shape(train_data))\n",
    "    print(np.shape(train_label))\n",
    "    return train_data, test_data, train_label, test_label, valid_data, valid_label\n",
    "\n",
    "\n",
    "\n",
    "# parse image                                                                                                               \n",
    "def img_to_matrix(filename, verbose=False):\n",
    "    img = Image.open(filename)\n",
    "    if verbose:\n",
    "        print('changing size from %s to %s' % (str(img.size), str(STANDARD_SIZE)))\n",
    "    img = img.resize(STANDARD_SIZE)\n",
    "    imgArray = np.asarray(img)\n",
    "    return imgArray  # imgArray.shape = (167 x 300 x 3)                                                                     \n",
    "\n",
    "\n",
    "# 1次元に引き延ばす(PCAで使用)                                                                                              \n",
    "def flatten_image(img):\n",
    "\n",
    "    s = img.shape[0] * img.shape[1] #* img.shape[2]\n",
    "    img_wide = img.reshape(1, s)\n",
    "    return img_wide[0]\n",
    "\n",
    "\n",
    "def handle_image_with_pca(activations, y_test):\n",
    "\n",
    "    for i in range(2):\n",
    "        images = activations[i,:,:,:]\n",
    "        print(images)\n",
    "        if i == 0:\n",
    "            labels = y_test[:64]\n",
    "        elif i == 1:\n",
    "            labels = y_test[64:]\n",
    "        ls = []\n",
    "\n",
    "        for label in labels:\n",
    "\n",
    "            if list(label) == [0,1]:\n",
    "                ls.append(\"cloudy_seesaa\")\n",
    "            elif list(label) == [1,0]:\n",
    "                ls.append(\"sunny_seesaa\")\n",
    "        labels = ls\n",
    "        print(labels)\n",
    "        data = []\n",
    "        for a in range(64):#for image in images:\n",
    "            img = flatten_image(images[:, :, a])\n",
    "            data.append(img)\n",
    "\n",
    "        data = np.array(data)\n",
    "\n",
    "        is_train = np.random.uniform(0, 1, len(data)) <= 0.7\n",
    "        y = np.where(np.array(labels) == 'cloudy_seesaa', 1, 0)\n",
    "\n",
    "        train_x, train_y = data[is_train], y[is_train]\n",
    "\n",
    "        pca = RandomizedPCA(n_components=2)\n",
    "        X = pca.fit_transform(data)\n",
    "        if i == 0:\n",
    "            df1 = pd.DataFrame({\"x\": X[:, 0], \"y\": X[:, 1],\n",
    "                               \"label\": np.where(y == 1, 'cloudy_seesaa', 'sunny_seesaa')})\n",
    "        elif i == 1:\n",
    "            df2 = pd.DataFrame({\"x\": X[:, 0], \"y\": X[:, 1],\n",
    "                               \"label\": np.where(y == 1, 'cloudy_seesaa', 'sunny_seesaa')})\n",
    "            df = pd.concat([df1, df2])\n",
    "    colors = ['red','blue']\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for label, color in zip(df['label'].unique(), colors):\n",
    "        mask = df['label'] == label\n",
    "        plt.scatter(df[mask]['x'], df[mask]['y'], c=color, label=label)\n",
    "    sns.set()\n",
    "    plt.xlabel(\"pc1 (Principal Component1)\")  # 全データの分散が最大となる方向                                              \n",
    "    plt.ylabel(\"pc2 (Principal Component2)\")  # 第一主成分に垂直な方向の軸                                                  \n",
    "    plt.legend()\n",
    "    #plt.show()                                                                                                             \n",
    "    plt.savefig('pca_feature1.png')\n",
    "\n",
    "    # training a classifier                                                                                                 \n",
    "    pca = RandomizedPCA(n_components=2)\n",
    "    train_x = pca.fit_transform(train_x)\n",
    "    \"\"\"\n",
    "    svm = LinearSVC(C=1.0)\n",
    "    svm.fit(train_x, train_y)\n",
    "    joblib.dump(svm, 'model.pkl')\n",
    "\n",
    "    # evaluating the model                                                                                                  \n",
    "    test_x, test_y = data[is_train == False], y[is_train == False]\n",
    "    test_x = pca.transform(test_x)\n",
    "    print(pd.crosstab(test_y, svm.predict(test_x),\n",
    "                      rownames=['Actual'], colnames=['Predicted']))\n",
    "    \"\"\"\n",
    "def main():\n",
    "\n",
    "    x_train, x_test, y_train, y_test, valid_data, valid_label = input_data(\"path_and_label_train.txt\",\n",
    "                                                                           \"path_and_label_test.txt\")\n",
    "\n",
    "    old_session = KTF.get_session()\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        session = tf.Session('')\n",
    "        KTF.set_session(session)\n",
    "        KTF.set_learning_phase(1)\n",
    "        \"\"\"\n",
    "        model = Sequential()                                                                                                \n",
    "        model.add(Conv2D(32, kernel_size=(3, 3),                                                                            \n",
    "                         activation='relu',                                                                                 \n",
    "                         input_shape=input_shape, kernel_initializer=\"he_normal\",                        \n",
    "                         bias_initializer=\"zeros\"))                                                                         \n",
    "        model.add(Conv2D(64, (3, 3), activation='relu'))                                                                    \n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Dropout(0.25))                                                                                            \n",
    "        model.add(Flatten())                                                                                                \n",
    "        model.add(Dense(28, activation='relu', init='he_uniform'))                                                          \n",
    "        model.add(Dropout(0.5))                                                                                             \n",
    "        model.add(Dense(num_classes, activation='softmax')) # num_classes = 2値分類                                         \n",
    "                                                                                                                            \n",
    "                                                                                                                            \n",
    "        \"\"\"\n",
    "       # load trained model                                                                                                \n",
    "        json_string = open(os.path.join(f_model, model_filename)).read()\n",
    "        model = model_from_json(json_string)\n",
    "        model.load_weights(os.path.join(f_model, weights_filename))\n",
    "        \"\"\"                                                                                                                 \n",
    "        model.compile(loss=keras.losses.categorical_crossentropy,                                                           \n",
    "                      optimizer=\"SGD\",                                                                                      \n",
    "                      metrics=['accuracy'])                                                                                 \n",
    "                                                                                                                            \n",
    "        print(model.summary())                                                                                              \n",
    "                                                                                                                            \n",
    "        # callback function                                                                                                 \n",
    "        tb_cb = keras.callbacks.TensorBoard(log_dir=f_log, histogram_freq=1)                                                \n",
    "        cbks = [tb_cb]                                                                                                      \n",
    "                                                                                                                            \n",
    "        # train                                                                                                             \n",
    "        history = model.fit(x_train, y_train,                                                                               \n",
    "                            batch_size=batch_size,                                                                          \n",
    "                            epochs=epochs,                                                                                  \n",
    "                            verbose=1,       #進行状況の表示モード                                                          \n",
    "                            callbacks=cbks,  # [plot_losses, csv_logger],                                                   \n",
    "                            validation_data=(x_test, y_test))                                                               \n",
    "        score_train = model.evaluate(x_train,y_train, verbose=1, batch_size=4)                                              \n",
    "        score_test = model.evaluate(x_test, y_test, verbose=1, batch_size=4)                                                \n",
    "        print('Train loss: {0}'.format(score_train[0]))                                                                     \n",
    "        print('Train accuracy: {0}'.format(score_train[1]))                                                                 \n",
    "        print('Test loss: {0}'.format(score_test[0]))                                                                       \n",
    "        print('Test accuracy: {0}'.format(score_test[1]))                                                                   \n",
    "                                                                                                                            \n",
    "        # 学習済みモデル書き出し                                                                                            \n",
    "        json_string = model.to_json()                                                                                       \n",
    "        open(os.path.join(f_model, 'cnn_model.json'), 'w').write(json_string)                                               \n",
    "        yaml_string = model.to_yaml()                                                                                       \n",
    "        open(os.path.join(f_model, 'cnn_model.yaml'), 'w').write(yaml_string)                                               \n",
    "        print('save weights')                                                                                               \n",
    "        model.save_weights(os.path.join(f_model, 'cnn_model_weights.hdf5'))                                                 \n",
    "        \"\"\"\n",
    "       # modelのlayer_nameを調べる                                                                                                                         \n",
    "        for layer in model.layers:\n",
    "            print(layer.name)\n",
    "\n",
    "        layer_name =\"dropout_1\"#\"conv2d_2\"# \"max_pooling2d_1\"                                                                                    \n",
    "        intermediate_layer_model = keras.models.Model(inputs=model.input,\n",
    "                                         outputs=model.get_layer(layer_name).output)\n",
    "\n",
    "\n",
    "        layers = model.layers[3:4]     \n",
    "        \n",
    "        urls = [\"/home/seimei/Graduation_Research/dataset_valid/hare/class3-1/image_0104.jpg\", \n",
    "        \"/home/seimei/Graduation_Research/dataset_valid/kumori/class4-5/image_0064.jpg\"]\n",
    "\n",
    "        activations = np.zeros((0,148,148,64))\n",
    "        for url in urls:\n",
    "            img = image.load_img(url, target_size=(300, 300))\n",
    "            img = image.img_to_array(img)\n",
    "            img /= 255\n",
    "            img = np.expand_dims(img, axis=0)\n",
    "            # 指定したlayer_nameと一致するレイヤーの出力を取得する                                                                                              \n",
    "            _activations = intermediate_layer_model.predict(img)\n",
    "            _activations = [activation for layer, activation in zip(layers, _activations) if isinstance(layer, Dropout)]\n",
    "            print(np.shape(_activations))\n",
    "            activations = np.r_[activations, np.reshape(_activations,(-1,148,148,64))]\n",
    "        print(np.shape(activations))\n",
    "        \"\"\"\n",
    "        # 単品の特徴画像生成#                                                                                                                               \n",
    "        for i, activation in enumerate(activations):\n",
    "            num_of_image = activation.shape[2]\n",
    "            max = np.max(activation[0])\n",
    "            for j in range(0, num_of_image):\n",
    "                plt.figure(figsize=(50, 50))\n",
    "                sns.heatmap(activation[:, :,j], vmin=0, vmax=max, xticklabels=False, yticklabels=False, square=False)\n",
    "                plt.savefig(\"%d_%d.png\" % (i+1, j+1))\n",
    "                plt.close()\n",
    "        # 出力層ごとに特徴画像を並べてヒートマップ画像として出力                                                                                            \n",
    "        for i, activation in enumerate(activations):\n",
    "            num_of_image = activation.shape[2]\n",
    "            cols = math.ceil(math.sqrt(num_of_image))\n",
    "            rows = math.floor(num_of_image / cols)\n",
    "            screen = []\n",
    "            for y in range(0, rows):\n",
    "                row = []\n",
    "                for x in range(0, cols):\n",
    "                    j = y * cols + x\n",
    "                    if j < num_of_image:\n",
    "                        row.append(activation[:, :, j])\n",
    "                    else:\n",
    "                        row.append(np.zeros())\n",
    "                screen.append(np.concatenate(row, axis=1))\n",
    "            screen = np.concatenate(screen, axis=0)\n",
    "            plt.figure(figsize=(50, 50))\n",
    "            sns.heatmap(screen, xticklabels=False, yticklabels=False)\n",
    "            name = \"maxpooling2d\"\n",
    "            plt.savefig(\"%s.png\" % name)\n",
    "            plt.close()\n",
    "        \"\"\"\n",
    "        y = keras.utils.to_categorical(np.r_[np.zeros(64), np.ones(64)], num_classes)\n",
    "\n",
    "        handle_image_with_pca(activations, y )\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    # add for TeonsorBoard                                                                                                                                  \n",
    "\n",
    "    KTF.set_session(old_session)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
